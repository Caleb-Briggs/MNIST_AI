{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a368ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST FILE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da1b1a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:14c6bcad-0e27-4c99-a8aa-fd9116ce4c20"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MNIST Cluster Size Sweep Experiment (v2)\n",
    "=========================================\n",
    "Paste this into Google Colab with A100 runtime.\n",
    "\n",
    "Improvements:\n",
    "- Uses all 60k MNIST examples\n",
    "- Clusters are strictly disjoint (no overlapping examples)\n",
    "- Parallel training of multiple models for small cluster sizes\n",
    "\"\"\"\n",
    "\n",
    "#%% Cell 1: Imports and Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "#%% Cell 2: Configuration\n",
    "class Config:\n",
    "    # Cluster sizes to sweep\n",
    "    cluster_sizes = [1, 2, 3, 5, 7, 10, 15, 20, 30, 50, 70, 100]\n",
    "\n",
    "    # Training settings\n",
    "    batch_size = 128\n",
    "    epochs = 20\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "\n",
    "    # How many clusters to train per cluster size\n",
    "    max_clusters_to_train = 200\n",
    "\n",
    "    # Parallel training: how many models to train simultaneously\n",
    "    # (for small cluster sizes, we train many models in parallel)\n",
    "    parallel_models = 50  # Adjust based on GPU memory\n",
    "\n",
    "    # Seed\n",
    "    seed = 123\n",
    "\n",
    "config = Config()\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "#%% Cell 3: Model Definition\n",
    "class SmallCNN(nn.Module):\n",
    "    \"\"\"Small CNN (~100k params)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def create_model():\n",
    "    return SmallCNN().to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in create_model().parameters())\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "\n",
    "#%% Cell 4: Load Data\n",
    "print(\"Loading MNIST...\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Extract ALL data into tensors\n",
    "all_images = train_dataset.data.float().unsqueeze(1) / 255.0\n",
    "all_images = (all_images - 0.1307) / 0.3081\n",
    "all_labels = train_dataset.targets\n",
    "all_images = all_images.to(device)\n",
    "all_labels = all_labels.to(device)\n",
    "\n",
    "num_total = len(all_labels)\n",
    "print(f\"Total examples: {num_total}\")\n",
    "\n",
    "#%% Cell 5: Create Disjoint Clusters for All Sizes\n",
    "print(\"\\nCreating disjoint clusters for all cluster sizes...\")\n",
    "\n",
    "# Single random permutation for all cluster assignments\n",
    "master_permutation = np.random.permutation(num_total)\n",
    "\n",
    "def create_disjoint_clusters(cluster_size, permutation):\n",
    "    \"\"\"Create disjoint clusters of given size from the permutation.\"\"\"\n",
    "    num_clusters = len(permutation) // cluster_size\n",
    "    clusters = []\n",
    "    for i in range(num_clusters):\n",
    "        start = i * cluster_size\n",
    "        end = start + cluster_size\n",
    "        clusters.append(permutation[start:end])\n",
    "    return clusters\n",
    "\n",
    "# Pre-create clusters for all sizes\n",
    "all_clusters = {}\n",
    "for size in config.cluster_sizes:\n",
    "    clusters = create_disjoint_clusters(size, master_permutation)\n",
    "    all_clusters[size] = clusters\n",
    "    print(f\"  Size {size:>3}: {len(clusters)} disjoint clusters\")\n",
    "\n",
    "#%% Cell 6: Sequential Training (for larger cluster sizes)\n",
    "def train_single_model(cluster_indices, epochs=config.epochs):\n",
    "    \"\"\"Train a single model on given indices.\"\"\"\n",
    "    model = create_model()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "    X = all_images[cluster_indices]\n",
    "    y = all_labels[cluster_indices]\n",
    "\n",
    "    actual_batch_size = min(config.batch_size, len(cluster_indices))\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=actual_batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_X, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = F.cross_entropy(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "#%% Cell 7: Parallel Training (for small cluster sizes)\n",
    "def train_models_parallel(list_of_cluster_indices, epochs=config.epochs):\n",
    "    \"\"\"\n",
    "    Train multiple models in parallel by interleaving gradient updates.\n",
    "    Returns list of trained models.\n",
    "    \"\"\"\n",
    "    num_models = len(list_of_cluster_indices)\n",
    "\n",
    "    # Create all models and optimizers\n",
    "    models = [create_model() for _ in range(num_models)]\n",
    "    optimizers = [torch.optim.AdamW(m.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "                  for m in models]\n",
    "    schedulers = [torch.optim.lr_scheduler.CosineAnnealingLR(opt, epochs)\n",
    "                  for opt in optimizers]\n",
    "\n",
    "    # Prepare data for each model\n",
    "    data_X = [all_images[idx] for idx in list_of_cluster_indices]\n",
    "    data_y = [all_labels[idx] for idx in list_of_cluster_indices]\n",
    "\n",
    "    # Training loop - interleave updates across models\n",
    "    for epoch in range(epochs):\n",
    "        # Set all models to train mode\n",
    "        for m in models:\n",
    "            m.train()\n",
    "\n",
    "        # For small clusters, we might only have 1-few examples per model\n",
    "        # So we just do one \"batch\" per epoch (the whole cluster)\n",
    "        for i in range(num_models):\n",
    "            X, y = data_X[i], data_y[i]\n",
    "\n",
    "            optimizers[i].zero_grad()\n",
    "            output = models[i](X)\n",
    "            loss = F.cross_entropy(output, y)\n",
    "            loss.backward()\n",
    "            optimizers[i].step()\n",
    "\n",
    "        for s in schedulers:\n",
    "            s.step()\n",
    "\n",
    "    return models\n",
    "\n",
    "#%% Cell 8: Evaluation Functions\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, indices):\n",
    "    \"\"\"Evaluate single model on given indices, return accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    X = all_images[indices]\n",
    "    y = all_labels[indices]\n",
    "\n",
    "    # Process in batches\n",
    "    batch_size = 2048\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_X = X[i:i+batch_size]\n",
    "        batch_y = y[i:i+batch_size]\n",
    "        output = model(batch_X)\n",
    "        preds = output.argmax(dim=1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += len(batch_y)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_models_on_clusters(models, clusters, train_cluster_indices):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models on the clusters we trained on (not all clusters).\n",
    "\n",
    "    Args:\n",
    "        models: list of M models\n",
    "        clusters: list of all clusters\n",
    "        train_cluster_indices: list of M cluster indices that each model was trained on\n",
    "\n",
    "    Returns:\n",
    "        accuracy_matrix: (M, M) array where entry [i,j] is accuracy of model i on cluster j's data\n",
    "                        (diagonal is NaN since we exclude self-evaluation)\n",
    "    \"\"\"\n",
    "    num_models = len(models)\n",
    "    accuracy_matrix = np.full((num_models, num_models), np.nan)\n",
    "\n",
    "    # Set all models to eval mode\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "\n",
    "    # Get data for each trained cluster (cache it)\n",
    "    cluster_data = []\n",
    "    for c_idx in train_cluster_indices:\n",
    "        c_indices = clusters[c_idx]\n",
    "        X = all_images[c_indices]\n",
    "        y = all_labels[c_indices]\n",
    "        cluster_data.append((X, y))\n",
    "\n",
    "    # Evaluate: model i on cluster j (where j indexes into train_cluster_indices)\n",
    "    for j, (X, y) in enumerate(cluster_data):\n",
    "        for i, model in enumerate(models):\n",
    "            if i == j:  # Skip self-evaluation\n",
    "                continue\n",
    "\n",
    "            output = model(X)\n",
    "            preds = output.argmax(dim=1)\n",
    "            acc = (preds == y).float().mean().item()\n",
    "            accuracy_matrix[i, j] = acc\n",
    "\n",
    "    return accuracy_matrix\n",
    "\n",
    "#%% Cell 9: Main Sweep Loop\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING CLUSTER SIZE SWEEP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sweep_results = {}\n",
    "\n",
    "for cluster_size in config.cluster_sizes:\n",
    "    clusters = all_clusters[cluster_size]\n",
    "    num_clusters = len(clusters)\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Cluster size: {cluster_size}, Total clusters: {num_clusters}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Decide how many clusters to train\n",
    "    num_to_train = min(num_clusters, config.max_clusters_to_train)\n",
    "\n",
    "    # Randomly select which clusters to train on\n",
    "    np.random.seed(config.seed + cluster_size)  # Reproducible selection per size\n",
    "    train_cluster_indices = np.random.choice(num_clusters, size=num_to_train, replace=False)\n",
    "\n",
    "    print(f\"Training {num_to_train} models...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Storage\n",
    "    all_models = []\n",
    "    all_train_cluster_idx = []\n",
    "\n",
    "    # Decide whether to use parallel or sequential training\n",
    "    if cluster_size <= 10:\n",
    "        # Use parallel training in batches\n",
    "        batch_size = config.parallel_models\n",
    "        num_batches = (num_to_train + batch_size - 1) // batch_size\n",
    "\n",
    "        for batch_idx in tqdm(range(num_batches), desc=f\"Size {cluster_size} (parallel)\"):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = min(batch_start + batch_size, num_to_train)\n",
    "            batch_train_indices = train_cluster_indices[batch_start:batch_end]\n",
    "\n",
    "            # Get cluster data for this batch\n",
    "            batch_clusters = [clusters[i] for i in batch_train_indices]\n",
    "\n",
    "            # Train models in parallel\n",
    "            batch_models = train_models_parallel(batch_clusters, epochs=config.epochs)\n",
    "\n",
    "            all_models.extend(batch_models)\n",
    "            all_train_cluster_idx.extend(batch_train_indices)\n",
    "\n",
    "            # Clear some memory\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        # Use sequential training for larger clusters\n",
    "        for i, train_idx in enumerate(tqdm(train_cluster_indices, desc=f\"Size {cluster_size} (sequential)\")):\n",
    "            model = train_single_model(clusters[train_idx], epochs=config.epochs)\n",
    "            all_models.append(model)\n",
    "            all_train_cluster_idx.append(train_idx)\n",
    "\n",
    "            # Periodic memory cleanup\n",
    "            if (i + 1) % 50 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Training completed in {train_time:.1f}s\")\n",
    "\n",
    "    # Evaluate all models on all trained clusters\n",
    "    print(\"Evaluating models on trained clusters...\")\n",
    "    eval_start = time.time()\n",
    "\n",
    "    accuracy_matrix = evaluate_models_on_clusters(all_models, clusters, all_train_cluster_idx)\n",
    "\n",
    "    eval_time = time.time() - eval_start\n",
    "    print(f\"Evaluation completed in {eval_time:.1f}s\")\n",
    "\n",
    "    # Compute diagonal (training set) accuracies - batch this too\n",
    "    diag_accs = []\n",
    "    for model in all_models:\n",
    "        model.eval()\n",
    "\n",
    "    for m_idx, train_idx in enumerate(all_train_cluster_idx):\n",
    "        X = all_images[clusters[train_idx]]\n",
    "        y = all_labels[clusters[train_idx]]\n",
    "        with torch.no_grad():\n",
    "            output = all_models[m_idx](X)\n",
    "            preds = output.argmax(dim=1)\n",
    "            acc = (preds == y).float().mean().item()\n",
    "        diag_accs.append(acc)\n",
    "    diag_accs = np.array(diag_accs)\n",
    "\n",
    "    # Compute off-diagonal statistics\n",
    "    off_diag_accs = accuracy_matrix[~np.isnan(accuracy_matrix)]\n",
    "\n",
    "    results = {\n",
    "        'cluster_size': cluster_size,\n",
    "        'num_clusters': num_clusters,\n",
    "        'num_trained': num_to_train,\n",
    "        'off_diag_mean': off_diag_accs.mean(),\n",
    "        'off_diag_std': off_diag_accs.std(),\n",
    "        'off_diag_median': np.median(off_diag_accs),\n",
    "        'off_diag_min': off_diag_accs.min(),\n",
    "        'off_diag_max': off_diag_accs.max(),\n",
    "        'off_diag_25': np.percentile(off_diag_accs, 25),\n",
    "        'off_diag_75': np.percentile(off_diag_accs, 75),\n",
    "        'diag_mean': diag_accs.mean(),\n",
    "        'diag_std': diag_accs.std(),\n",
    "        'all_off_diag': off_diag_accs,\n",
    "        'all_diag': diag_accs,\n",
    "        'matrix': accuracy_matrix,\n",
    "        'train_cluster_indices': np.array(all_train_cluster_idx),\n",
    "        'train_time': train_time,\n",
    "        'eval_time': eval_time\n",
    "    }\n",
    "\n",
    "    sweep_results[cluster_size] = results\n",
    "\n",
    "    print(f\"  Off-diagonal accuracy: {results['off_diag_mean']:.4f} ± {results['off_diag_std']:.4f}\")\n",
    "    print(f\"  Diagonal (train) accuracy: {results['diag_mean']:.4f} ± {results['diag_std']:.4f}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del all_models\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "#%% Cell 10: Summary Plot - Accuracy vs Cluster Size\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "sizes = config.cluster_sizes\n",
    "means = [sweep_results[s]['off_diag_mean'] for s in sizes]\n",
    "stds = [sweep_results[s]['off_diag_std'] for s in sizes]\n",
    "q25 = [sweep_results[s]['off_diag_25'] for s in sizes]\n",
    "q75 = [sweep_results[s]['off_diag_75'] for s in sizes]\n",
    "\n",
    "# Plot 1: Mean accuracy vs cluster size\n",
    "ax = axes[0, 0]\n",
    "ax.errorbar(sizes, means, yerr=stds, fmt='o-', capsize=5, capthick=2, linewidth=2, markersize=8, label='Mean ± Std')\n",
    "ax.fill_between(sizes, q25, q75, alpha=0.3, label='25th-75th percentile')\n",
    "ax.set_xlabel('Cluster Size (training examples)', fontsize=12)\n",
    "ax.set_ylabel('Off-Diagonal Accuracy', fontsize=12)\n",
    "ax.set_title('Generalization Accuracy vs Training Cluster Size', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(sizes)\n",
    "ax.set_xticklabels(sizes)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0.1, color='red', linestyle='--', alpha=0.5, label='Random (10%)')\n",
    "\n",
    "# Plot 2: Box plots\n",
    "ax = axes[0, 1]\n",
    "box_data = [sweep_results[s]['all_off_diag'] for s in sizes]\n",
    "bp = ax.boxplot(box_data, labels=sizes, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "ax.set_xlabel('Cluster Size', fontsize=12)\n",
    "ax.set_ylabel('Off-Diagonal Accuracy', fontsize=12)\n",
    "ax.set_title('Distribution of Generalization Accuracies', fontsize=14)\n",
    "ax.axhline(y=0.1, color='red', linestyle='--', alpha=0.5)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Train vs Test accuracy\n",
    "ax = axes[1, 0]\n",
    "train_means = [sweep_results[s]['diag_mean'] for s in sizes]\n",
    "test_means = [sweep_results[s]['off_diag_mean'] for s in sizes]\n",
    "ax.plot(sizes, train_means, 'o-', linewidth=2, markersize=8, label='Training (diagonal)')\n",
    "ax.plot(sizes, test_means, 's-', linewidth=2, markersize=8, label='Test (off-diagonal)')\n",
    "ax.set_xlabel('Cluster Size', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Train vs Test Accuracy', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xticks(sizes)\n",
    "ax.set_xticklabels(sizes)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Generalization gap\n",
    "ax = axes[1, 1]\n",
    "gaps = [t - e for t, e in zip(train_means, test_means)]\n",
    "ax.bar(range(len(sizes)), gaps, tick_label=sizes, color='coral', edgecolor='black')\n",
    "ax.set_xlabel('Cluster Size', fontsize=12)\n",
    "ax.set_ylabel('Train - Test Accuracy Gap', fontsize=12)\n",
    "ax.set_title('Generalization Gap', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster_size_sweep_summary.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "#%% Cell 11: Violin Plots\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "violin_data = [sweep_results[s]['all_off_diag'] for s in sizes]\n",
    "parts = ax.violinplot(violin_data, positions=range(len(sizes)), showmeans=True, showmedians=True)\n",
    "\n",
    "for pc in parts['bodies']:\n",
    "    pc.set_facecolor('steelblue')\n",
    "    pc.set_alpha(0.7)\n",
    "\n",
    "ax.set_xticks(range(len(sizes)))\n",
    "ax.set_xticklabels(sizes)\n",
    "ax.set_xlabel('Cluster Size (training examples)', fontsize=12)\n",
    "ax.set_ylabel('Off-Diagonal Accuracy', fontsize=12)\n",
    "ax.set_title('Distribution of Generalization Accuracies by Cluster Size', fontsize=14)\n",
    "ax.axhline(y=0.1, color='red', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cluster_size_violins.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "#%% Cell 12: Heatmaps for Selected Sizes\n",
    "key_sizes = [5, 20, 50, 100]\n",
    "key_sizes = [s for s in key_sizes if s in sweep_results]\n",
    "\n",
    "if len(key_sizes) > 0:\n",
    "    fig, axes = plt.subplots(1, len(key_sizes), figsize=(5*len(key_sizes), 5))\n",
    "    if len(key_sizes) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, size in zip(axes, key_sizes):\n",
    "        matrix = sweep_results[size]['matrix']\n",
    "        show_size = min(50, matrix.shape[0], matrix.shape[1])\n",
    "        matrix_show = matrix[:show_size, :show_size].copy()\n",
    "\n",
    "        # Diagonal is already NaN from evaluation\n",
    "\n",
    "        im = ax.imshow(matrix_show, cmap='viridis', aspect='auto')\n",
    "        ax.set_title(f'Cluster Size = {size}\\n({show_size}x{show_size}, diagonal excluded)')\n",
    "        ax.set_xlabel('Eval Cluster (index into trained)')\n",
    "        ax.set_ylabel('Train Model')\n",
    "        plt.colorbar(im, ax=ax, label='Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cluster_size_heatmaps.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "#%% Cell 13: Learning Curve Fit\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sizes_arr = np.array(sizes)\n",
    "means_arr = np.array(means)\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def learning_curve(n, a, b, c):\n",
    "    return a - b * np.exp(-c * n)\n",
    "\n",
    "try:\n",
    "    popt, pcov = curve_fit(learning_curve, sizes_arr, means_arr, p0=[0.9, 0.8, 0.1], maxfev=10000)\n",
    "    fitted_sizes = np.linspace(1, 100, 200)\n",
    "    fitted_acc = learning_curve(fitted_sizes, *popt)\n",
    "\n",
    "    ax.scatter(sizes, means, s=100, zorder=5, label='Observed')\n",
    "    ax.plot(fitted_sizes, fitted_acc, 'r-', linewidth=2,\n",
    "            label=f'Fit: {popt[0]:.3f} - {popt[1]:.3f}·exp(-{popt[2]:.3f}·n)')\n",
    "\n",
    "    # Estimate cluster size needed for various accuracy thresholds\n",
    "    thresholds = [0.5, 0.6, 0.7, 0.8]\n",
    "    for thresh in thresholds:\n",
    "        if popt[0] > thresh and popt[1] > 0:\n",
    "            ratio = (popt[0] - thresh) / popt[1]\n",
    "            if ratio > 0:\n",
    "                n_thresh = -np.log(ratio) / popt[2]\n",
    "                if 1 <= n_thresh <= 100:\n",
    "                    ax.axhline(y=thresh, color='gray', linestyle=':', alpha=0.5)\n",
    "                    ax.axvline(x=n_thresh, color='gray', linestyle=':', alpha=0.5)\n",
    "                    ax.annotate(f'{thresh*100:.0f}% @ n≈{n_thresh:.0f}',\n",
    "                               xy=(n_thresh, thresh), xytext=(n_thresh+5, thresh-0.03),\n",
    "                               fontsize=10)\n",
    "except Exception as e:\n",
    "    print(f\"Curve fitting failed: {e}\")\n",
    "    ax.scatter(sizes, means, s=100, zorder=5, label='Observed')\n",
    "\n",
    "ax.set_xlabel('Cluster Size (training examples)', fontsize=12)\n",
    "ax.set_ylabel('Mean Off-Diagonal Accuracy', fontsize=12)\n",
    "ax.set_title('Learning Curve: Accuracy vs Training Set Size', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curve_fit.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "#%% Cell 14: Timing Analysis\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "train_times = [sweep_results[s]['train_time'] for s in sizes]\n",
    "eval_times = [sweep_results[s]['eval_time'] for s in sizes]\n",
    "\n",
    "x = np.arange(len(sizes))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, train_times, width, label='Training', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, eval_times, width, label='Evaluation', color='coral')\n",
    "\n",
    "ax.set_xlabel('Cluster Size', fontsize=12)\n",
    "ax.set_ylabel('Time (seconds)', fontsize=12)\n",
    "ax.set_title('Compute Time by Cluster Size', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sizes)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('timing_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "#%% Cell 15: Summary Table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'Size':>6} | {'Clusters':>8} | {'Trained':>7} | {'Test Acc':>14} | {'Train Acc':>14} | {'Gap':>8} | {'Time':>10}\")\n",
    "print(\"-\" * 90)\n",
    "for s in sizes:\n",
    "    r = sweep_results[s]\n",
    "    gap = r['diag_mean'] - r['off_diag_mean']\n",
    "    total_time = r['train_time'] + r['eval_time']\n",
    "    print(f\"{s:>6} | {r['num_clusters']:>8} | {r['num_trained']:>7} | \"\n",
    "          f\"{r['off_diag_mean']:.4f}±{r['off_diag_std']:.4f} | \"\n",
    "          f\"{r['diag_mean']:.4f}±{r['diag_std']:.4f} | {gap:>8.4f} | {total_time:>9.1f}s\")\n",
    "\n",
    "#%% Cell 16: Save Results\n",
    "import json\n",
    "\n",
    "results_to_save = {\n",
    "    'config': {\n",
    "        'cluster_sizes': config.cluster_sizes,\n",
    "        'epochs': config.epochs,\n",
    "        'max_clusters_to_train': config.max_clusters_to_train,\n",
    "        'parallel_models': config.parallel_models,\n",
    "        'seed': config.seed\n",
    "    },\n",
    "    'sweep_results': {\n",
    "        str(size): {\n",
    "            'cluster_size': r['cluster_size'],\n",
    "            'num_clusters': r['num_clusters'],\n",
    "            'num_trained': r['num_trained'],\n",
    "            'off_diag_mean': float(r['off_diag_mean']),\n",
    "            'off_diag_std': float(r['off_diag_std']),\n",
    "            'off_diag_median': float(r['off_diag_median']),\n",
    "            'diag_mean': float(r['diag_mean']),\n",
    "            'diag_std': float(r['diag_std']),\n",
    "            'train_time': r['train_time'],\n",
    "            'eval_time': r['eval_time']\n",
    "        }\n",
    "        for size, r in sweep_results.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('cluster_size_sweep_results.json', 'w') as f:\n",
    "    json.dump(results_to_save, f, indent=2)\n",
    "\n",
    "# Save full arrays\n",
    "np.savez_compressed('cluster_size_sweep_full.npz',\n",
    "    **{f'size_{s}_off_diag': sweep_results[s]['all_off_diag'] for s in sizes},\n",
    "    **{f'size_{s}_diag': sweep_results[s]['all_diag'] for s in sizes},\n",
    "    **{f'size_{s}_matrix': sweep_results[s]['matrix'] for s in sizes},\n",
    "    **{f'size_{s}_train_idx': sweep_results[s]['train_cluster_indices'] for s in sizes}\n",
    ")\n",
    "\n",
    "print(\"\\nResults saved!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc12b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
