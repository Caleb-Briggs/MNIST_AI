{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Model Search: Generalization Distribution\n",
    "\n",
    "**Goal**: Test the distribution of generalization among random CNN models.\n",
    "\n",
    "**Key question**: Among models that achieve >20% train accuracy (above chance), how many generalize?\n",
    "\n",
    "Runs indefinitely until stopped. Saves periodically (overwriting old saves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from dataclasses import dataclass, field\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Data splits\n",
    "    'train_size': 256,\n",
    "    'val_size': 5000,\n",
    "    'test_size': 5000,\n",
    "    'data_seed': 42,\n",
    "    \n",
    "    # Search parameters\n",
    "    'batch_size': 256,  # Models per batch (smaller for CNN memory)\n",
    "    'screen_threshold': 0.20,\n",
    "    \n",
    "    # Weight initialization\n",
    "    'weight_scale': 1.5,\n",
    "    'bias_scale': 0.5,\n",
    "    \n",
    "    # Saving\n",
    "    'save_every': 50000,\n",
    "    'log_every': 10000,  # Print progress every N models\n",
    "    'max_best_weights': 25,\n",
    "    'results_file': 'results/search_results.json',\n",
    "    'weights_file': 'results/best_weights.pt',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Best Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_best_random(n_samples: int, n_trials: int, p: float = 0.1) -> float:\n",
    "    \"\"\"Expected best accuracy from n_trials random models.\"\"\"\n",
    "    if n_trials <= 1:\n",
    "        return p\n",
    "    mu = p\n",
    "    sigma = math.sqrt(p * (1 - p) / n_samples)\n",
    "    expected_max = mu + sigma * math.sqrt(2 * math.log(n_trials))\n",
    "    return min(expected_max, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(device):\n",
    "    \"\"\"Load MNIST as 1x28x28 images.\"\"\"\n",
    "    from torchvision import datasets, transforms\n",
    "    \n",
    "    train_data = datasets.MNIST(\n",
    "        root='./data', train=True, download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "    )\n",
    "    \n",
    "    images = torch.stack([img for img, _ in train_data]).to(device)  # (N, 1, 28, 28)\n",
    "    labels = torch.tensor([lbl for _, lbl in train_data]).to(device)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def create_fixed_splits(images, labels, train_size, val_size, test_size, seed):\n",
    "    \"\"\"Create fixed, non-overlapping splits.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    indices = rng.permutation(len(images))\n",
    "    \n",
    "    train_idx = indices[:train_size]\n",
    "    val_idx = indices[train_size:train_size + val_size]\n",
    "    test_idx = indices[train_size + val_size:train_size + val_size + test_size]\n",
    "    \n",
    "    return (\n",
    "        (images[train_idx], labels[train_idx]),\n",
    "        (images[val_idx], labels[val_idx]),\n",
    "        (images[test_idx], labels[test_idx]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading MNIST...\")\n",
    "all_images, all_labels = load_mnist(device)\n",
    "print(f\"Loaded {len(all_images)} images\")\n",
    "\n",
    "(train_images, train_labels), (val_images, val_labels), (test_images, test_labels) = \\\n",
    "    create_fixed_splits(\n",
    "        all_images, all_labels,\n",
    "        CONFIG['train_size'],\n",
    "        CONFIG['val_size'],\n",
    "        CONFIG['test_size'],\n",
    "        CONFIG['data_seed']\n",
    "    )\n",
    "\n",
    "print(f\"Train: {len(train_images)}, Val: {len(val_images)}, Test: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small CNN Model\n",
    "\n",
    "Tiny CNN for fast random search:\n",
    "- Conv1: 1 → 4 channels, 5x5, stride 2 → 12x12\n",
    "- Conv2: 4 → 8 channels, 5x5, stride 2 → 4x4  \n",
    "- FC: 128 → 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyCNN(nn.Module):\n",
    "    \"\"\"Tiny CNN for random search.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, stride=2)   # -> 4x12x12\n",
    "        self.conv2 = nn.Conv2d(4, 8, 5, stride=2)   # -> 8x4x4\n",
    "        self.fc = nn.Linear(8 * 4 * 4, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "def randomize_model(model, weight_scale, bias_scale):\n",
    "    \"\"\"Randomize all parameters with given scales.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                param.normal_(0, bias_scale)\n",
    "            else:\n",
    "                param.normal_(0, weight_scale)\n",
    "\n",
    "\n",
    "def get_model_params(model):\n",
    "    \"\"\"Get model parameters as dict of CPU tensors.\"\"\"\n",
    "    return {name: param.cpu().clone() for name, param in model.named_parameters()}\n",
    "\n",
    "\n",
    "def evaluate_model(model, images, labels):\n",
    "    \"\"\"Compute accuracy.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits = model(images)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        return (preds == labels).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "test_model = TinyCNN().to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "randomize_model(test_model, CONFIG['weight_scale'], CONFIG['bias_scale'])\n",
    "test_acc = evaluate_model(test_model, train_images, train_labels)\n",
    "print(f\"Random model train accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SearchResults:\n",
    "    \"\"\"Track search results.\"\"\"\n",
    "    config: dict = field(default_factory=dict)\n",
    "    n_failed_screen: int = 0\n",
    "    passing_results: list = field(default_factory=list)  # [train_acc, val_acc]\n",
    "    best_results: list = field(default_factory=list)  # [train, val, test, exp_best, idx]\n",
    "    best_val_acc: float = 0.0\n",
    "    best_model_idx: int = -1\n",
    "    total_evaluated: int = 0\n",
    "    elapsed_seconds: float = 0.0\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'config': self.config,\n",
    "            'n_failed_screen': self.n_failed_screen,\n",
    "            'passing_results': self.passing_results,\n",
    "            'best_results': self.best_results,\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'best_model_idx': self.best_model_idx,\n",
    "            'total_evaluated': self.total_evaluated,\n",
    "            'elapsed_seconds': self.elapsed_seconds,\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, d):\n",
    "        return cls(\n",
    "            config=d.get('config', {}),\n",
    "            n_failed_screen=d.get('n_failed_screen', 0),\n",
    "            passing_results=d.get('passing_results', []),\n",
    "            best_results=d.get('best_results', []),\n",
    "            best_val_acc=d.get('best_val_acc', 0.0),\n",
    "            best_model_idx=d.get('best_model_idx', -1),\n",
    "            total_evaluated=d.get('total_evaluated', 0),\n",
    "            elapsed_seconds=d.get('elapsed_seconds', 0.0),\n",
    "        )\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        Path(filepath).parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Write to temp file first, then rename (atomic)\n",
    "        temp_path = filepath + '.tmp'\n",
    "        with open(temp_path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f)\n",
    "            f.flush()\n",
    "        Path(temp_path).rename(filepath)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            return cls.from_dict(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Search Loop (Runs Indefinitely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_search(\n",
    "    train_data: tuple,\n",
    "    val_data: tuple,\n",
    "    test_data: tuple,\n",
    "    config: dict,\n",
    "    resume_from: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run random model search indefinitely.\n",
    "    Saves periodically. Stop with Ctrl+C or runtime disconnect.\n",
    "    \"\"\"\n",
    "    train_images, train_labels = train_data\n",
    "    val_images, val_labels = val_data\n",
    "    test_images, test_labels = test_data\n",
    "    \n",
    "    # Initialize or load\n",
    "    results_file = config['results_file']\n",
    "    weights_file = config['weights_file']\n",
    "    \n",
    "    if resume_from and Path(resume_from).exists():\n",
    "        print(f\"Resuming from {resume_from}\")\n",
    "        results = SearchResults.load(resume_from)\n",
    "    else:\n",
    "        results = SearchResults(config=config)\n",
    "    \n",
    "    # Load existing best weights\n",
    "    if Path(weights_file).exists():\n",
    "        best_weights_list = torch.load(weights_file, weights_only=False)\n",
    "        best_weights = deque(best_weights_list, maxlen=config['max_best_weights'])\n",
    "        print(f\"Loaded {len(best_weights)} best model weights\")\n",
    "    else:\n",
    "        best_weights = deque(maxlen=config['max_best_weights'])\n",
    "    \n",
    "    batch_size = config['batch_size']\n",
    "    save_every = config['save_every']\n",
    "    log_every = config['log_every']\n",
    "    threshold = config['screen_threshold']\n",
    "    weight_scale = config['weight_scale']\n",
    "    bias_scale = config['bias_scale']\n",
    "    \n",
    "    # Create model template\n",
    "    model = TinyCNN().to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    last_save_count = results.total_evaluated\n",
    "    last_log_count = results.total_evaluated\n",
    "    \n",
    "    print(f\"\\nStarting search (runs indefinitely, Ctrl+C to stop)\")\n",
    "    print(f\"Total evaluated so far: {results.total_evaluated:,}\")\n",
    "    print(f\"Passing models so far: {len(results.passing_results):,}\")\n",
    "    print(f\"Current best val acc: {results.best_val_acc:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Evaluate batch of random models\n",
    "            for _ in range(batch_size):\n",
    "                randomize_model(model, weight_scale, bias_scale)\n",
    "                \n",
    "                # Screen on train\n",
    "                train_acc = evaluate_model(model, train_images, train_labels)\n",
    "                \n",
    "                if train_acc < threshold:\n",
    "                    results.n_failed_screen += 1\n",
    "                else:\n",
    "                    # Passed screening - evaluate on val\n",
    "                    val_acc = evaluate_model(model, val_images, val_labels)\n",
    "                    results.passing_results.append([train_acc, val_acc])\n",
    "                    \n",
    "                    # Check if new best\n",
    "                    if val_acc > results.best_val_acc:\n",
    "                        test_acc = evaluate_model(model, test_images, test_labels)\n",
    "                        exp_best = expected_best_random(len(val_images), results.total_evaluated + 1)\n",
    "                        \n",
    "                        results.best_val_acc = val_acc\n",
    "                        results.best_model_idx = results.total_evaluated\n",
    "                        results.best_results.append([\n",
    "                            train_acc, val_acc, test_acc, exp_best, results.total_evaluated\n",
    "                        ])\n",
    "                        \n",
    "                        # Save weights\n",
    "                        best_weights.append({\n",
    "                            'params': get_model_params(model),\n",
    "                            'model_idx': results.total_evaluated,\n",
    "                            'val_acc': val_acc,\n",
    "                        })\n",
    "                        \n",
    "                        # Log new best\n",
    "                        print(f\"[NEW BEST #{len(results.best_results)}] \"\n",
    "                              f\"model {results.total_evaluated:,} | \"\n",
    "                              f\"train: {train_acc:.3f} | \"\n",
    "                              f\"val: {val_acc:.3f} | \"\n",
    "                              f\"test: {test_acc:.3f} | \"\n",
    "                              f\"exp_best: {exp_best:.3f}\")\n",
    "                \n",
    "                results.total_evaluated += 1\n",
    "            \n",
    "            # Periodic logging\n",
    "            if results.total_evaluated - last_log_count >= log_every:\n",
    "                elapsed = time.time() - start_time + results.elapsed_seconds\n",
    "                rate = results.total_evaluated / elapsed if elapsed > 0 else 0\n",
    "                pass_rate = len(results.passing_results) / results.total_evaluated\n",
    "                print(f\"[PROGRESS] {results.total_evaluated:,} models | \"\n",
    "                      f\"{rate:,.0f}/sec | \"\n",
    "                      f\"passing: {len(results.passing_results):,} ({pass_rate:.3%}) | \"\n",
    "                      f\"best: {results.best_val_acc:.3f}\")\n",
    "                last_log_count = results.total_evaluated\n",
    "            \n",
    "            # Periodic save\n",
    "            if results.total_evaluated - last_save_count >= save_every:\n",
    "                results.elapsed_seconds = time.time() - start_time + results.elapsed_seconds\n",
    "                results.save(results_file)\n",
    "                \n",
    "                Path(weights_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "                temp_weights = weights_file + '.tmp'\n",
    "                torch.save(list(best_weights), temp_weights)\n",
    "                Path(temp_weights).rename(weights_file)\n",
    "                \n",
    "                print(f\"[SAVED] {results_file}\")\n",
    "                last_save_count = results.total_evaluated\n",
    "                start_time = time.time()  # Reset for next interval\n",
    "                results.elapsed_seconds = results.elapsed_seconds  # Keep accumulated\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n\\n{'='*60}\")\n",
    "        print(\"Search interrupted. Saving final state...\")\n",
    "    \n",
    "    finally:\n",
    "        # Final save\n",
    "        results.elapsed_seconds += time.time() - start_time\n",
    "        results.save(results_file)\n",
    "        torch.save(list(best_weights), weights_file)\n",
    "        print(f\"Saved to {results_file}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FINAL RESULTS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total evaluated: {results.total_evaluated:,}\")\n",
    "        print(f\"Passed screening: {len(results.passing_results):,} ({len(results.passing_results)/max(1,results.total_evaluated):.3%})\")\n",
    "        print(f\"Best val accuracy: {results.best_val_acc:.4f}\")\n",
    "        print(f\"Best models found: {len(results.best_results)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_search(\n",
    "    (train_images, train_labels),\n",
    "    (val_images, val_labels),\n",
    "    (test_images, test_labels),\n",
    "    CONFIG,\n",
    "    resume_from=CONFIG['results_file']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_search_results(results: SearchResults):\n",
    "    \"\"\"Visualize search results.\"\"\"\n",
    "    if not results.passing_results:\n",
    "        print(\"No passing models to analyze\")\n",
    "        return\n",
    "    \n",
    "    passing = np.array(results.passing_results)\n",
    "    train_acc = passing[:, 0]\n",
    "    val_acc = passing[:, 1]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Generalization gap\n",
    "    ax = axes[0, 0]\n",
    "    gap = train_acc - val_acc\n",
    "    ax.hist(gap, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(0, color='red', linestyle='--', label='No gap')\n",
    "    ax.axvline(gap.mean(), color='green', linestyle='--', label=f'Mean: {gap.mean():.3f}')\n",
    "    ax.set_xlabel('Generalization Gap (Train - Val)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'Generalization Gap (n={len(gap):,})')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 2. Train vs Val scatter\n",
    "    ax = axes[0, 1]\n",
    "    ax.scatter(train_acc, val_acc, alpha=0.3, s=5)\n",
    "    if results.best_results:\n",
    "        best = np.array(results.best_results)\n",
    "        ax.scatter(best[:, 0], best[:, 1], color='red', s=30, \n",
    "                   label='Best models', zorder=5, edgecolor='black')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
    "    ax.set_xlabel('Train Accuracy')\n",
    "    ax.set_ylabel('Val Accuracy')\n",
    "    ax.set_title('Train vs Val Accuracy')\n",
    "    ax.legend()\n",
    "    \n",
    "    # 3. Best model progress\n",
    "    ax = axes[1, 0]\n",
    "    if results.best_results:\n",
    "        best = np.array(results.best_results)\n",
    "        ax.plot(best[:, 4], best[:, 1], 'b-o', markersize=4, label='Val Acc')\n",
    "        ax.plot(best[:, 4], best[:, 2], 'g-o', markersize=4, label='Test Acc')\n",
    "        ax.plot(best[:, 4], best[:, 3], 'r--', linewidth=2, label='Expected Best (random)')\n",
    "        ax.axhline(0.1, color='gray', linestyle=':', alpha=0.5)\n",
    "        ax.set_xlabel('Model Index')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_title('Best Model Progress vs Random Baseline')\n",
    "        ax.legend()\n",
    "        ax.set_xscale('log')\n",
    "    \n",
    "    # 4. Val accuracy distribution\n",
    "    ax = axes[1, 1]\n",
    "    ax.hist(val_acc, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(0.1, color='gray', linestyle=':', label='Chance (10%)')\n",
    "    ax.axvline(val_acc.mean(), color='green', linestyle='--', label=f'Mean: {val_acc.mean():.3f}')\n",
    "    ax.set_xlabel('Validation Accuracy')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Val Accuracy Distribution')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Stats\n",
    "    print(f\"\\nTotal: {results.total_evaluated:,} | Passing: {len(results.passing_results):,}\")\n",
    "    print(f\"Val acc: mean={val_acc.mean():.4f}, max={val_acc.max():.4f}\")\n",
    "    print(f\"Generalizing (val>10%): {(val_acc > 0.1).sum():,} ({(val_acc > 0.1).mean():.2%})\")\n",
    "    if results.best_results:\n",
    "        best = np.array(results.best_results)\n",
    "        print(f\"Best: train={best[-1,0]:.3f}, val={best[-1,1]:.3f}, test={best[-1,2]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot (can run this cell separately after stopping search)\n",
    "if Path(CONFIG['results_file']).exists():\n",
    "    results = SearchResults.load(CONFIG['results_file'])\n",
    "    plot_search_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(CONFIG['results_file'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
