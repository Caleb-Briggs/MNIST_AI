{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Video Prediction - Training\n",
    "\n",
    "Train a transformer-based model to predict physics simulation frames.\n",
    "\n",
    "**Architecture**: CNN Encoder → Transformer → CNN Decoder\n",
    "\n",
    "**Goal**: Model learns to compress video to latent state that captures physics (position, velocity, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MNIST_AI'...\n",
      "remote: Enumerating objects: 136, done.\u001b[K\n",
      "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
      "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
      "remote: Total 136 (delta 34), reused 46 (delta 16), pack-reused 72 (from 1)\u001b[K\n",
      "Receiving objects: 100% (136/136), 22.93 MiB | 27.99 MiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n",
      "/content/MNIST_AI/experiments/physics_prediction/MNIST_AI/experiments/physics_prediction/MNIST_AI/experiments/physics_prediction/MNIST_AI/experiments/physics_prediction/MNIST_AI/experiments/physics_prediction\n"
     ]
    }
   ],
   "source": [
    "!rm -rf MNIST_AI  # Clean up any existing clone\n",
    "!git clone https://github.com/Caleb-Briggs/MNIST_AI.git\n",
    "%cd MNIST_AI/experiments/physics_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from physics_sim import (\n",
    "    Ball, Barrier, PhysicsSimulation,\n",
    "    generate_trajectory, create_random_simulation, generate_dataset\n",
    ")\n",
    "from model import VideoPredictor, count_parameters\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data config\nNUM_TRAJECTORIES = 1000  # Start smaller for faster iteration\nFRAMES_PER_TRAJECTORY = 50\nNUM_BARRIERS = 3\nWITH_GRAVITY = False  # Start simple\n\n# Model config  \nLATENT_DIM = 256\nN_HEADS = 4\nN_LAYERS = 4\nCONTEXT_LEN = 8  # Fixed context length for now (simpler)\n\n# Training config\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-4\nNUM_EPOCHS = 50\nSEED = 42\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\nprint(f\"Context length: {CONTEXT_LEN} frames\")\nprint(f\"Training: predict next frame from context\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data...\n",
      "Dataset shape: (2000, 50, 64, 64)\n",
      "Memory: 1562.5 MB\n",
      "Tensor shape: torch.Size([2000, 50, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating training data...\")\n",
    "data = generate_dataset(\n",
    "    num_trajectories=NUM_TRAJECTORIES,\n",
    "    num_frames=FRAMES_PER_TRAJECTORY,\n",
    "    num_barriers=NUM_BARRIERS,\n",
    "    with_gravity=WITH_GRAVITY,\n",
    "    base_seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Memory: {data.nbytes / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Convert to torch tensor\n",
    "data_tensor = torch.from_numpy(data).float().unsqueeze(2)  # Add channel dim: (N, T, 1, H, W)\n",
    "print(f\"Tensor shape: {data_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few trajectories\n",
    "fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "for row in range(3):\n",
    "    for col in range(8):\n",
    "        t = col * 6  # 0, 6, 12, 18, 24, 30, 36, 42 (fits in 50 frames)\n",
    "        axes[row, col].imshow(data[row, t], cmap='gray', vmin=0, vmax=1)\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(f't={t}')\n",
    "        axes[row, col].axis('off')\n",
    "plt.suptitle('Sample Trajectories')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class PhysicsDataset(torch.utils.data.Dataset):\n    \"\"\"Simple dataset: fixed context length, predict next frame.\"\"\"\n    \n    def __init__(self, trajectories: torch.Tensor, context_len: int):\n        \"\"\"\n        Args:\n            trajectories: (num_traj, num_frames, 1, H, W)\n            context_len: number of context frames\n        \"\"\"\n        self.trajectories = trajectories\n        self.context_len = context_len\n        self.num_traj = trajectories.size(0)\n        self.num_frames = trajectories.size(1)\n        \n        # Each trajectory provides multiple samples\n        self.samples_per_traj = self.num_frames - context_len\n        \n    def __len__(self):\n        return self.num_traj * self.samples_per_traj\n    \n    def __getitem__(self, idx):\n        traj_idx = idx // self.samples_per_traj\n        frame_idx = idx % self.samples_per_traj\n        \n        # Context: frames [frame_idx : frame_idx + context_len]\n        # Target: frame [frame_idx + context_len]\n        context = self.trajectories[traj_idx, frame_idx:frame_idx + self.context_len]\n        target = self.trajectories[traj_idx, frame_idx + self.context_len]\n        \n        return context, target\n\n\n# Split into train/val\ntrain_size = int(0.9 * NUM_TRAJECTORIES)\ntrain_data = data_tensor[:train_size]\nval_data = data_tensor[train_size:]\n\ntrain_dataset = PhysicsDataset(train_data, CONTEXT_LEN)\nval_dataset = PhysicsDataset(val_data, CONTEXT_LEN)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n    num_workers=2, pin_memory=True\n)\nval_loader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=2, pin_memory=True\n)\n\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Val samples: {len(val_dataset)}\")\nprint(f\"Each sample: {CONTEXT_LEN} context frames -> 1 target frame\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model = VideoPredictor(\n    latent_dim=LATENT_DIM,\n    n_heads=N_HEADS,\n    n_layers=N_LAYERS,\n    dim_feedforward=LATENT_DIM * 2,\n    dropout=0.1\n).to(device)\n\nprint(f\"Model parameters: {count_parameters(model):,}\")\n\n# Test forward pass\nsample_context, sample_target = next(iter(train_loader))\nsample_context = sample_context.to(device)\n\nwith torch.no_grad():\n    sample_pred = model(sample_context)\n\nprint(f\"Context shape: {sample_context.shape}\")\nprint(f\"Target shape: {sample_target.shape}\")\nprint(f\"Prediction shape: {sample_pred.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "# Loss function: MSE on pixels\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'lr': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(model, loader, optimizer, criterion, device):\n    \"\"\"Train for one epoch - simple next-frame prediction.\"\"\"\n    model.train()\n    total_loss = 0\n    num_batches = 0\n    \n    for context, target in loader:\n        context = context.to(device)  # (batch, context_len, 1, 64, 64)\n        target = target.to(device)    # (batch, 1, 64, 64)\n        \n        optimizer.zero_grad()\n        \n        pred = model(context)  # (batch, 1, 64, 64)\n        loss = criterion(pred, target)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n        optimizer.step()\n        \n        total_loss += loss.item()\n        num_batches += 1\n    \n    return total_loss / num_batches\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion, device):\n    \"\"\"Evaluate for one epoch.\"\"\"\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    \n    for context, target in loader:\n        context = context.to(device)\n        target = target.to(device)\n        \n        pred = model(context)\n        loss = criterion(pred, target)\n        \n        total_loss += loss.item()\n        num_batches += 1\n    \n    return total_loss / num_batches\n\n\n@torch.no_grad()\ndef quick_visualize(model, val_data, device, epoch, context_len=8):\n    \"\"\"Quick visualization during training.\"\"\"\n    model.eval()\n    \n    fig, axes = plt.subplots(3, 5, figsize=(12, 7))\n    \n    for row in range(3):\n        traj_idx = row * 30  # Different trajectories\n        \n        # Get context and predict\n        context = val_data[traj_idx, :context_len].unsqueeze(0).to(device)\n        target = val_data[traj_idx, context_len]\n        pred = model(context).cpu().squeeze(0)\n        \n        # Show last 2 context frames\n        axes[row, 0].imshow(context[0, -2, 0].cpu(), cmap='gray', vmin=0, vmax=1)\n        axes[row, 0].set_title('Ctx t-1' if row == 0 else '')\n        axes[row, 0].axis('off')\n        \n        axes[row, 1].imshow(context[0, -1, 0].cpu(), cmap='gray', vmin=0, vmax=1)\n        axes[row, 1].set_title('Ctx t' if row == 0 else '')\n        axes[row, 1].axis('off')\n        \n        # Show prediction\n        axes[row, 2].imshow(pred[0].clamp(0, 1), cmap='gray', vmin=0, vmax=1)\n        axes[row, 2].set_title('Predicted' if row == 0 else '')\n        axes[row, 2].axis('off')\n        \n        # Show ground truth\n        axes[row, 3].imshow(target[0], cmap='gray', vmin=0, vmax=1)\n        axes[row, 3].set_title('Ground Truth' if row == 0 else '')\n        axes[row, 3].axis('off')\n        \n        # Show error\n        error = torch.abs(pred[0] - target[0])\n        axes[row, 4].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n        axes[row, 4].set_title('Error' if row == 0 else '')\n        axes[row, 4].axis('off')\n    \n    plt.suptitle(f'Epoch {epoch}')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualization frequency\nVIS_EVERY = 10  # Show visualizations every N epochs\n\nprint(f\"Training for {NUM_EPOCHS} epochs...\")\nprint(f\"Context: {CONTEXT_LEN} frames -> predict next frame\")\nprint(\"=\"*60)\n\nfor epoch in range(NUM_EPOCHS):\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    val_loss = eval_epoch(model, val_loader, criterion, device)\n    \n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['lr'].append(optimizer.param_groups[0]['lr'])\n    \n    scheduler.step()\n    \n    # Print progress\n    if (epoch + 1) % 5 == 0 or epoch == 0:\n        print(f\"Epoch {epoch+1:3d}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}\")\n    \n    # Periodic visualization\n    if (epoch + 1) % VIS_EVERY == 0 or epoch == 0:\n        quick_visualize(model, val_data, device, epoch + 1, CONTEXT_LEN)\n\nprint(\"=\"*60)\nprint(\"Training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(history['lr'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Step Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef visualize_predictions(model, val_data, device, num_samples=5, context_len=8):\n    \"\"\"Show context, prediction, ground truth, and error.\"\"\"\n    model.eval()\n    \n    fig, axes = plt.subplots(num_samples, 5, figsize=(12, 2.5*num_samples))\n    \n    indices = np.random.choice(len(val_data), num_samples, replace=False)\n    \n    for row, traj_idx in enumerate(indices):\n        # Random starting point in trajectory\n        start = np.random.randint(0, len(val_data[traj_idx]) - context_len - 1)\n        \n        context = val_data[traj_idx, start:start+context_len].unsqueeze(0).to(device)\n        target = val_data[traj_idx, start+context_len]\n        pred = model(context).cpu().squeeze(0)\n        \n        # Show last 2 context frames\n        axes[row, 0].imshow(context[0, -2, 0].cpu(), cmap='gray', vmin=0, vmax=1)\n        axes[row, 0].set_title('Ctx t-1' if row == 0 else '')\n        axes[row, 0].axis('off')\n        \n        axes[row, 1].imshow(context[0, -1, 0].cpu(), cmap='gray', vmin=0, vmax=1)\n        axes[row, 1].set_title('Ctx t' if row == 0 else '')\n        axes[row, 1].axis('off')\n        \n        # Prediction\n        axes[row, 2].imshow(pred[0].clamp(0, 1), cmap='gray', vmin=0, vmax=1)\n        axes[row, 2].set_title('Predicted' if row == 0 else '')\n        axes[row, 2].axis('off')\n        \n        # Ground truth\n        axes[row, 3].imshow(target[0], cmap='gray', vmin=0, vmax=1)\n        axes[row, 3].set_title('Ground Truth' if row == 0 else '')\n        axes[row, 3].axis('off')\n        \n        # Error\n        error = torch.abs(pred[0] - target[0])\n        axes[row, 4].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n        axes[row, 4].set_title('Error' if row == 0 else '')\n        axes[row, 4].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_predictions(model, val_data, device, num_samples=5, context_len=CONTEXT_LEN)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef visualize_rollout(model, trajectories, device, traj_idx=0, context_len=8, rollout_steps=20):\n    \"\"\"Compare autoregressive rollout with ground truth.\"\"\"\n    model.eval()\n    \n    # Get initial context\n    context = trajectories[traj_idx, :context_len].unsqueeze(0).to(device)\n    \n    # Ground truth future\n    gt_future = trajectories[traj_idx, context_len:context_len+rollout_steps].cpu().numpy()\n    \n    # Autoregressive rollout\n    predicted = model.rollout(context, rollout_steps).cpu().squeeze(0).numpy()\n    \n    # Visualize\n    num_show = min(8, rollout_steps)\n    step_indices = np.linspace(0, rollout_steps-1, num_show, dtype=int)\n    \n    fig, axes = plt.subplots(3, num_show, figsize=(2*num_show, 6))\n    \n    for col, t in enumerate(step_indices):\n        # Predicted\n        axes[0, col].imshow(np.clip(predicted[t, 0], 0, 1), cmap='gray', vmin=0, vmax=1)\n        if col == 0:\n            axes[0, col].set_ylabel('Predicted')\n        axes[0, col].set_title(f't+{t+1}')\n        axes[0, col].axis('off')\n        \n        # Ground truth\n        axes[1, col].imshow(gt_future[t, 0], cmap='gray', vmin=0, vmax=1)\n        if col == 0:\n            axes[1, col].set_ylabel('Ground Truth')\n        axes[1, col].axis('off')\n        \n        # Error\n        error = np.abs(np.clip(predicted[t, 0], 0, 1) - gt_future[t, 0])\n        axes[2, col].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n        if col == 0:\n            axes[2, col].set_ylabel('Error')\n        axes[2, col].axis('off')\n    \n    plt.suptitle(f'Autoregressive Rollout ({rollout_steps} steps)')\n    plt.tight_layout()\n    plt.show()\n    \n    # Compute MSE over time\n    mse_over_time = np.mean((np.clip(predicted[:, 0], 0, 1) - gt_future[:, 0])**2, axis=(1, 2))\n    \n    plt.figure(figsize=(8, 4))\n    plt.plot(range(1, rollout_steps+1), mse_over_time)\n    plt.xlabel('Rollout Step')\n    plt.ylabel('MSE')\n    plt.title('Prediction Error vs Rollout Length')\n    plt.grid(alpha=0.3)\n    plt.show()\n\n# Test rollout\nfor traj_idx in [0, 30, 60]:\n    print(f\"\\nTrajectory {traj_idx}:\")\n    visualize_rollout(model, val_data, device, traj_idx=traj_idx, context_len=CONTEXT_LEN, rollout_steps=20)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animated Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef animate_rollout_comparison(model, trajectories, device, traj_idx=0, context_len=8, rollout_steps=30):\n    \"\"\"Create side-by-side animation of prediction vs ground truth.\"\"\"\n    model.eval()\n    \n    # Get data\n    context = trajectories[traj_idx, :context_len].unsqueeze(0).to(device)\n    gt_future = trajectories[traj_idx, context_len:context_len+rollout_steps].cpu().numpy()\n    predicted = model.rollout(context, rollout_steps).cpu().squeeze(0).numpy()\n    \n    # Create animation\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    \n    im_pred = axes[0].imshow(np.clip(predicted[0, 0], 0, 1), cmap='gray', vmin=0, vmax=1)\n    axes[0].set_title('Predicted')\n    axes[0].axis('off')\n    \n    im_gt = axes[1].imshow(gt_future[0, 0], cmap='gray', vmin=0, vmax=1)\n    axes[1].set_title('Ground Truth')\n    axes[1].axis('off')\n    \n    error = np.abs(np.clip(predicted[0, 0], 0, 1) - gt_future[0, 0])\n    im_err = axes[2].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n    axes[2].set_title('Error')\n    axes[2].axis('off')\n    \n    title = fig.suptitle('t+1')\n    \n    def update(frame):\n        im_pred.set_array(np.clip(predicted[frame, 0], 0, 1))\n        im_gt.set_array(gt_future[frame, 0])\n        error = np.abs(np.clip(predicted[frame, 0], 0, 1) - gt_future[frame, 0])\n        im_err.set_array(error)\n        title.set_text(f't+{frame+1}')\n        return [im_pred, im_gt, im_err, title]\n    \n    anim = animation.FuncAnimation(fig, update, frames=rollout_steps, interval=100, blit=False)\n    plt.close()\n    return HTML(anim.to_jshtml())\n\nanimate_rollout_comparison(model, val_data, device, traj_idx=0, context_len=CONTEXT_LEN, rollout_steps=30)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis: Does the Model Learn Physics?\n",
    "\n",
    "### Latent Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def analyze_latent_space(model, trajectories, device, num_traj=5):\n",
    "    \"\"\"Analyze what the latent space encodes.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode several trajectories\n",
    "    all_latents = []\n",
    "    for i in range(num_traj):\n",
    "        traj = trajectories[i].unsqueeze(0).to(device)  # (1, T, 1, H, W)\n",
    "        latents = model.encode_frames(traj).cpu().numpy()  # (1, T, latent_dim)\n",
    "        all_latents.append(latents[0])\n",
    "    \n",
    "    # Plot latent trajectories (first 3 dimensions)\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    ax1 = fig.add_subplot(131)\n",
    "    for i, latents in enumerate(all_latents):\n",
    "        ax1.plot(latents[:, 0], label=f'Traj {i}')\n",
    "    ax1.set_xlabel('Frame')\n",
    "    ax1.set_ylabel('Latent dim 0')\n",
    "    ax1.set_title('Latent Dimension 0 Over Time')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    ax2 = fig.add_subplot(132)\n",
    "    for i, latents in enumerate(all_latents):\n",
    "        ax2.plot(latents[:, 1], label=f'Traj {i}')\n",
    "    ax2.set_xlabel('Frame')\n",
    "    ax2.set_ylabel('Latent dim 1')\n",
    "    ax2.set_title('Latent Dimension 1 Over Time')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # 2D projection of latent trajectory\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    for i, latents in enumerate(all_latents):\n",
    "        ax3.plot(latents[:, 0], latents[:, 1], 'o-', markersize=2, alpha=0.7, label=f'Traj {i}')\n",
    "        ax3.plot(latents[0, 0], latents[0, 1], 'go', markersize=8)  # Start\n",
    "        ax3.plot(latents[-1, 0], latents[-1, 1], 'ro', markersize=8)  # End\n",
    "    ax3.set_xlabel('Latent dim 0')\n",
    "    ax3.set_ylabel('Latent dim 1')\n",
    "    ax3.set_title('Latent Space Trajectory')\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_latent_space(model, val_data, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save model checkpoint\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'history': history,\n    'config': {\n        'latent_dim': LATENT_DIM,\n        'n_heads': N_HEADS,\n        'n_layers': N_LAYERS,\n        'context_len': CONTEXT_LEN,\n    }\n}\ntorch.save(checkpoint, 'results/model_checkpoint.pt')\nprint(\"Model saved to results/model_checkpoint.pt\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key metrics to track:\n",
    "- **Single-step MSE**: How well does model predict 1 frame ahead?\n",
    "- **Rollout degradation**: How fast does error grow with longer rollouts?\n",
    "- **Visual quality**: Do predictions look like valid physics?\n",
    "- **Latent structure**: Does latent space encode position/velocity?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}