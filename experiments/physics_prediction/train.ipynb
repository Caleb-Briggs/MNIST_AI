{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Video Prediction - Training\n",
    "\n",
    "Train a transformer-based model to predict physics simulation frames.\n",
    "\n",
    "**Architecture**: CNN Encoder → Transformer → CNN Decoder\n",
    "\n",
    "**Goal**: Model learns to compress video to latent state that captures physics (position, velocity, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MNIST_AI'...\n",
      "remote: Enumerating objects: 136, done.\u001b[K\n",
      "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
      "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
      "remote: Total 136 (delta 34), reused 46 (delta 16), pack-reused 72 (from 1)\u001b[K\n",
      "Receiving objects: 100% (136/136), 22.93 MiB | 27.99 MiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n",
      "/content/MNIST_AI/experiments/physics_prediction/MNIST_AI/experiments/physics_prediction/MNIST_AI/experiments/physics_prediction/MNIST_AI/experiments/physics_prediction/MNIST_AI/experiments/physics_prediction\n"
     ]
    }
   ],
   "source": [
    "# Clone repo (for Colab)\n",
    "!git clone https://github.com/Caleb-Briggs/MNIST_AI.git\n",
    "%cd MNIST_AI/experiments/physics_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from physics_sim import (\n",
    "    Ball, Barrier, PhysicsSimulation,\n",
    "    generate_trajectory, create_random_simulation, generate_dataset\n",
    ")\n",
    "from model import VideoPredictor, count_parameters\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length: 4 to 16 frames\n",
      "Predict 3 steps from each context\n"
     ]
    }
   ],
   "source": [
    "# Data config\n",
    "NUM_TRAJECTORIES = 2000  # Many trajectories for diverse terrains\n",
    "FRAMES_PER_TRAJECTORY = 50  # Enough frames for context + rollout\n",
    "NUM_BARRIERS = 3\n",
    "WITH_GRAVITY = False  # Start simple\n",
    "\n",
    "# Model config\n",
    "LATENT_DIM = 256\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 4\n",
    "MIN_CONTEXT = 4   # Minimum context frames\n",
    "MAX_CONTEXT = 16  # Maximum context frames  \n",
    "PREDICT_STEPS = 3  # Always predict this many steps ahead\n",
    "\n",
    "# Training config\n",
    "BATCH_SIZE = 64  # Smaller batch since samples have variable sizes\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 50\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Context length: {MIN_CONTEXT} to {MAX_CONTEXT} frames\")\n",
    "print(f\"Predict {PREDICT_STEPS} steps from each context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data...\n",
      "Dataset shape: (2000, 50, 64, 64)\n",
      "Memory: 1562.5 MB\n",
      "Tensor shape: torch.Size([2000, 50, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating training data...\")\n",
    "data = generate_dataset(\n",
    "    num_trajectories=NUM_TRAJECTORIES,\n",
    "    num_frames=FRAMES_PER_TRAJECTORY,\n",
    "    num_barriers=NUM_BARRIERS,\n",
    "    with_gravity=WITH_GRAVITY,\n",
    "    base_seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Memory: {data.nbytes / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Convert to torch tensor\n",
    "data_tensor = torch.from_numpy(data).float().unsqueeze(2)  # Add channel dim: (N, T, 1, H, W)\n",
    "print(f\"Tensor shape: {data_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize a few trajectories\nfig, axes = plt.subplots(3, 8, figsize=(16, 6))\nfor row in range(3):\n    for col in range(8):\n        t = col * 6  # 0, 6, 12, 18, 24, 30, 36, 42 (fits in 50 frames)\n        axes[row, col].imshow(data[row, t], cmap='gray', vmin=0, vmax=1)\n        if row == 0:\n            axes[row, col].set_title(f't={t}')\n        axes[row, col].axis('off')\nplt.suptitle('Sample Trajectories')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset with variable-length context windows.\n",
    "    \n",
    "    For each trajectory, we create multiple samples:\n",
    "    - Context frames 0:4, predict 4:7\n",
    "    - Context frames 0:5, predict 5:8\n",
    "    - Context frames 0:6, predict 6:9\n",
    "    - ... up to max_context\n",
    "    \n",
    "    This trains the model to handle varying context lengths.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trajectories: torch.Tensor, min_context: int, max_context: int, predict_steps: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trajectories: (num_traj, num_frames, 1, H, W)\n",
    "            min_context: minimum number of context frames\n",
    "            max_context: maximum number of context frames\n",
    "            predict_steps: number of future frames to predict\n",
    "        \"\"\"\n",
    "        self.trajectories = trajectories\n",
    "        self.min_context = min_context\n",
    "        self.max_context = max_context\n",
    "        self.predict_steps = predict_steps\n",
    "        self.num_traj = trajectories.size(0)\n",
    "        self.num_frames = trajectories.size(1)\n",
    "        \n",
    "        # Build index of all valid (trajectory, context_len) pairs\n",
    "        self.samples = []\n",
    "        for traj_idx in range(self.num_traj):\n",
    "            for ctx_len in range(min_context, max_context + 1):\n",
    "                # Need ctx_len context frames + predict_steps target frames\n",
    "                if ctx_len + predict_steps <= self.num_frames:\n",
    "                    self.samples.append((traj_idx, ctx_len))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        traj_idx, ctx_len = self.samples[idx]\n",
    "        \n",
    "        # Context: frames 0 to ctx_len (variable length)\n",
    "        # Targets: frames ctx_len to ctx_len + predict_steps\n",
    "        context = self.trajectories[traj_idx, :ctx_len]  # (ctx_len, 1, 64, 64)\n",
    "        targets = self.trajectories[traj_idx, ctx_len:ctx_len + self.predict_steps]  # (predict_steps, 1, 64, 64)\n",
    "        \n",
    "        return context, targets\n",
    "\n",
    "\n",
    "def collate_variable_context(batch):\n",
    "    \"\"\"Custom collate function to pad variable-length contexts.\"\"\"\n",
    "    contexts, targets = zip(*batch)\n",
    "    \n",
    "    # Find max context length in this batch\n",
    "    max_ctx = max(c.size(0) for c in contexts)\n",
    "    \n",
    "    # Pad contexts to max length (pad at the beginning with zeros)\n",
    "    padded_contexts = []\n",
    "    context_lengths = []\n",
    "    for ctx in contexts:\n",
    "        ctx_len = ctx.size(0)\n",
    "        context_lengths.append(ctx_len)\n",
    "        if ctx_len < max_ctx:\n",
    "            # Pad at beginning: [0, 0, ..., actual_frames]\n",
    "            padding = torch.zeros(max_ctx - ctx_len, *ctx.shape[1:])\n",
    "            padded = torch.cat([padding, ctx], dim=0)\n",
    "        else:\n",
    "            padded = ctx\n",
    "        padded_contexts.append(padded)\n",
    "    \n",
    "    # Stack into batches\n",
    "    contexts_batch = torch.stack(padded_contexts)  # (batch, max_ctx, 1, 64, 64)\n",
    "    targets_batch = torch.stack(targets)  # (batch, predict_steps, 1, 64, 64)\n",
    "    lengths_batch = torch.tensor(context_lengths)  # (batch,)\n",
    "    \n",
    "    return contexts_batch, targets_batch, lengths_batch\n",
    "\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.9 * NUM_TRAJECTORIES)\n",
    "train_data = data_tensor[:train_size]\n",
    "val_data = data_tensor[train_size:]\n",
    "\n",
    "train_dataset = PhysicsDataset(train_data, MIN_CONTEXT, MAX_CONTEXT, PREDICT_STEPS)\n",
    "val_dataset = PhysicsDataset(val_data, MIN_CONTEXT, MAX_CONTEXT, PREDICT_STEPS)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "    num_workers=4, pin_memory=True, collate_fn=collate_variable_context\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=4, pin_memory=True, collate_fn=collate_variable_context\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)} ({train_size} trajectories × {MAX_CONTEXT - MIN_CONTEXT + 1} context lengths)\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(f\"Each sample: variable context ({MIN_CONTEXT}-{MAX_CONTEXT} frames) -> {PREDICT_STEPS} target frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(batch_size, max_len, actual_lengths, device):\n",
    "    \"\"\"Create padding mask: True for padded positions (at the beginning).\"\"\"\n",
    "    mask = torch.zeros(batch_size, max_len, dtype=torch.bool, device=device)\n",
    "    for i, length in enumerate(actual_lengths):\n",
    "        pad_len = max_len - length\n",
    "        if pad_len > 0:\n",
    "            mask[i, :pad_len] = True\n",
    "    return mask\n",
    "\n",
    "\n",
    "model = VideoPredictor(\n",
    "    latent_dim=LATENT_DIM,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    dim_feedforward=LATENT_DIM * 2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Test forward pass with variable context\n",
    "sample_context, sample_target, sample_lengths = next(iter(train_loader))\n",
    "sample_context = sample_context.to(device)\n",
    "sample_lengths = sample_lengths.to(device)\n",
    "\n",
    "# Create padding mask for test\n",
    "batch_size = sample_context.size(0)\n",
    "max_ctx = sample_context.size(1)\n",
    "padding_mask = create_padding_mask(batch_size, max_ctx, sample_lengths, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_pred = model(sample_context, n_future=PREDICT_STEPS, padding_mask=padding_mask)\n",
    "\n",
    "print(f\"Context shape: {sample_context.shape}\")\n",
    "print(f\"Context lengths: min={sample_lengths.min().item()}, max={sample_lengths.max().item()}\")\n",
    "print(f\"Output shape: {sample_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "# Loss function: MSE on pixels\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'lr': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device, predict_steps):\n",
    "    \"\"\"Train with variable-length context and parallel multi-step prediction.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for context, targets, lengths in loader:\n",
    "        context = context.to(device)  # (batch, max_ctx, 1, 64, 64)\n",
    "        targets = targets.to(device)  # (batch, predict_steps, 1, 64, 64)\n",
    "        lengths = lengths.to(device)  # (batch,)\n",
    "        \n",
    "        batch_size = context.size(0)\n",
    "        max_ctx = context.size(1)\n",
    "        \n",
    "        # Create padding mask\n",
    "        padding_mask = create_padding_mask(batch_size, max_ctx, lengths, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Predict future frames with padding mask\n",
    "        preds = model(context, n_future=predict_steps, padding_mask=padding_mask)\n",
    "        \n",
    "        # MSE loss over all predicted frames\n",
    "        loss = criterion(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "    \n",
    "    return total_loss / total_samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, criterion, device, predict_steps):\n",
    "    \"\"\"Evaluate with variable-length context.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for context, targets, lengths in loader:\n",
    "        context = context.to(device)\n",
    "        targets = targets.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        \n",
    "        batch_size = context.size(0)\n",
    "        max_ctx = context.size(1)\n",
    "        \n",
    "        padding_mask = create_padding_mask(batch_size, max_ctx, lengths, device)\n",
    "        preds = model(context, n_future=predict_steps, padding_mask=padding_mask)\n",
    "        \n",
    "        loss = criterion(preds, targets)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "    \n",
    "    return total_loss / total_samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quick_visualize(model, val_data, device, epoch, min_ctx=4, max_ctx=16):\n",
    "    \"\"\"Quick visualization during training - shows predictions at different context lengths.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 6, figsize=(15, 7.5))\n",
    "    \n",
    "    # Row 0: Short context (min_ctx frames)\n",
    "    # Row 1: Medium context (mid frames)  \n",
    "    # Row 2: Long context (max_ctx frames)\n",
    "    context_lengths = [min_ctx, (min_ctx + max_ctx) // 2, max_ctx]\n",
    "    \n",
    "    for row, ctx_len in enumerate(context_lengths):\n",
    "        # Get context from first trajectory\n",
    "        context = val_data[0, :ctx_len].unsqueeze(0).to(device)\n",
    "        target = val_data[0, ctx_len:ctx_len+3]  # 3 target frames\n",
    "        \n",
    "        # Predict\n",
    "        pred = model(context, n_future=3).cpu().squeeze(0)  # (3, 1, 64, 64)\n",
    "        \n",
    "        # Show last 2 context frames\n",
    "        for i in range(2):\n",
    "            frame_idx = ctx_len - 2 + i\n",
    "            axes[row, i].imshow(context[0, -2+i, 0].cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "            axes[row, i].set_title(f'Ctx t={frame_idx}' if row == 0 else '')\n",
    "            axes[row, i].axis('off')\n",
    "        \n",
    "        # Show 2 predictions vs ground truth (interleaved)\n",
    "        for t in range(2):\n",
    "            # Predicted\n",
    "            axes[row, 2 + t*2].imshow(pred[t, 0].clamp(0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "            axes[row, 2 + t*2].set_title(f'Pred t+{t+1}' if row == 0 else '')\n",
    "            axes[row, 2 + t*2].axis('off')\n",
    "            \n",
    "            # Ground truth\n",
    "            axes[row, 3 + t*2].imshow(target[t, 0], cmap='gray', vmin=0, vmax=1)\n",
    "            axes[row, 3 + t*2].set_title(f'GT t+{t+1}' if row == 0 else '')\n",
    "            axes[row, 3 + t*2].axis('off')\n",
    "        \n",
    "        # Label rows\n",
    "        axes[row, 0].set_ylabel(f'ctx={ctx_len}', fontsize=12)\n",
    "    \n",
    "    plt.suptitle(f'Epoch {epoch} - Predictions at Different Context Lengths')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization frequency\n",
    "VIS_EVERY = 10  # Show visualizations every N epochs\n",
    "\n",
    "print(f\"Training for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Variable context: {MIN_CONTEXT}-{MAX_CONTEXT} frames\")\n",
    "print(f\"Predict {PREDICT_STEPS} steps ahead\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device, PREDICT_STEPS)\n",
    "    val_loss = eval_epoch(model, val_loader, criterion, device, PREDICT_STEPS)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}\")\n",
    "    \n",
    "    # Periodic visualization\n",
    "    if (epoch + 1) % VIS_EVERY == 0 or epoch == 0:\n",
    "        quick_visualize(model, val_data, device, epoch + 1, MIN_CONTEXT, MAX_CONTEXT)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(history['lr'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Step Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_predictions(model, val_data, device, num_samples=5, context_len=8, num_future=5):\n",
    "    \"\"\"Show context, predictions at different horizons, and ground truth.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Columns: last 3 context frames + predicted/GT pairs\n",
    "    num_cols = 3 + num_future * 2\n",
    "    fig, axes = plt.subplots(num_samples, num_cols, figsize=(2*num_cols, 2.5*num_samples))\n",
    "    \n",
    "    indices = np.random.choice(len(val_data), num_samples, replace=False)\n",
    "    \n",
    "    for row, traj_idx in enumerate(indices):\n",
    "        context = val_data[traj_idx, :context_len].unsqueeze(0).to(device)\n",
    "        targets = val_data[traj_idx, context_len:context_len + num_future]\n",
    "        preds = model(context, n_future=num_future).cpu().squeeze(0)\n",
    "        \n",
    "        col = 0\n",
    "        # Show last 3 context frames\n",
    "        for i in range(3):\n",
    "            axes[row, col].imshow(context[0, -(3-i), 0].cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "            if row == 0:\n",
    "                axes[row, col].set_title(f'Ctx t-{2-i}')\n",
    "            axes[row, col].axis('off')\n",
    "            col += 1\n",
    "        \n",
    "        # Show predicted and ground truth frames side by side\n",
    "        for t in range(num_future):\n",
    "            # Predicted\n",
    "            axes[row, col].imshow(preds[t, 0].clamp(0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "            if row == 0:\n",
    "                axes[row, col].set_title(f'Pred t+{t+1}')\n",
    "            axes[row, col].axis('off')\n",
    "            col += 1\n",
    "            \n",
    "            # Ground truth\n",
    "            axes[row, col].imshow(targets[t, 0], cmap='gray', vmin=0, vmax=1)\n",
    "            if row == 0:\n",
    "                axes[row, col].set_title(f'GT t+{t+1}')\n",
    "            axes[row, col].axis('off')\n",
    "            col += 1\n",
    "    \n",
    "    plt.suptitle(f'Context ({context_len} frames) → Predictions vs Ground Truth')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test at different context lengths\n",
    "for ctx_len in [MIN_CONTEXT, (MIN_CONTEXT + MAX_CONTEXT) // 2, MAX_CONTEXT]:\n",
    "    print(f\"\\nContext length: {ctx_len}\")\n",
    "    visualize_predictions(model, val_data, device, num_samples=3, context_len=ctx_len, num_future=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_rollout(model, trajectories, device, traj_idx=0, context_len=8, rollout_steps=30):\n",
    "    \"\"\"Compare rollout with ground truth at a specific context length.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get initial context\n",
    "    context = trajectories[traj_idx, :context_len].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Ground truth future\n",
    "    gt_future = trajectories[traj_idx, context_len:context_len+rollout_steps].cpu().numpy()\n",
    "    \n",
    "    # Predict all future frames at once\n",
    "    predicted = model.rollout(context, rollout_steps).cpu().squeeze().numpy()\n",
    "    \n",
    "    # Visualize\n",
    "    num_show = min(10, rollout_steps)\n",
    "    step_indices = np.linspace(0, rollout_steps-1, num_show, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_show, figsize=(2*num_show, 6))\n",
    "    \n",
    "    for col, t in enumerate(step_indices):\n",
    "        # Predicted\n",
    "        axes[0, col].imshow(predicted[t, 0], cmap='gray', vmin=0, vmax=1)\n",
    "        if col == 0:\n",
    "            axes[0, col].set_ylabel('Predicted')\n",
    "        axes[0, col].set_title(f't+{t+1}')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[1, col].imshow(gt_future[t, 0], cmap='gray', vmin=0, vmax=1)\n",
    "        if col == 0:\n",
    "            axes[1, col].set_ylabel('Ground Truth')\n",
    "        axes[1, col].axis('off')\n",
    "        \n",
    "        # Error\n",
    "        error = np.abs(predicted[t, 0] - gt_future[t, 0])\n",
    "        axes[2, col].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n",
    "        if col == 0:\n",
    "            axes[2, col].set_ylabel('Error')\n",
    "        axes[2, col].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Rollout ({rollout_steps} steps from {context_len} context frames)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute MSE over time\n",
    "    mse_over_time = np.mean((predicted[:, 0] - gt_future[:, 0])**2, axis=(1, 2))\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, rollout_steps+1), mse_over_time)\n",
    "    plt.xlabel('Rollout Step')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title(f'Prediction Error vs Rollout Length (context={context_len})')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Test at different context lengths\n",
    "for ctx_len in [MIN_CONTEXT, MAX_CONTEXT]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Context length: {ctx_len}\")\n",
    "    visualize_rollout(model, val_data, device, traj_idx=0, context_len=ctx_len, rollout_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animated Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def animate_rollout_comparison(model, trajectories, device, traj_idx=0, context_len=8, rollout_steps=40):\n",
    "    \"\"\"Create side-by-side animation of prediction vs ground truth.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get data\n",
    "    context = trajectories[traj_idx, :context_len].unsqueeze(0).to(device)\n",
    "    gt_future = trajectories[traj_idx, context_len:context_len+rollout_steps].cpu().numpy()\n",
    "    predicted = model.rollout(context, rollout_steps).cpu().squeeze().numpy()\n",
    "    \n",
    "    # Create animation\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    im_pred = axes[0].imshow(predicted[0, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0].set_title('Predicted')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    im_gt = axes[1].imshow(gt_future[0, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1].set_title('Ground Truth')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    error = np.abs(predicted[0, 0] - gt_future[0, 0])\n",
    "    im_err = axes[2].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n",
    "    axes[2].set_title('Error')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    title = fig.suptitle(f'Context={context_len}, t+1')\n",
    "    \n",
    "    def update(frame):\n",
    "        im_pred.set_array(predicted[frame, 0])\n",
    "        im_gt.set_array(gt_future[frame, 0])\n",
    "        error = np.abs(predicted[frame, 0] - gt_future[frame, 0])\n",
    "        im_err.set_array(error)\n",
    "        title.set_text(f'Context={context_len}, t+{frame+1}')\n",
    "        return [im_pred, im_gt, im_err, title]\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, update, frames=rollout_steps, interval=100, blit=False)\n",
    "    plt.close()\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "animate_rollout_comparison(model, val_data, device, traj_idx=0, context_len=MAX_CONTEXT, rollout_steps=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis: Does the Model Learn Physics?\n",
    "\n",
    "### Latent Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def analyze_latent_space(model, trajectories, device, num_traj=5):\n",
    "    \"\"\"Analyze what the latent space encodes.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode several trajectories\n",
    "    all_latents = []\n",
    "    for i in range(num_traj):\n",
    "        traj = trajectories[i].unsqueeze(0).to(device)  # (1, T, 1, H, W)\n",
    "        latents = model.encode_frames(traj).cpu().numpy()  # (1, T, latent_dim)\n",
    "        all_latents.append(latents[0])\n",
    "    \n",
    "    # Plot latent trajectories (first 3 dimensions)\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    ax1 = fig.add_subplot(131)\n",
    "    for i, latents in enumerate(all_latents):\n",
    "        ax1.plot(latents[:, 0], label=f'Traj {i}')\n",
    "    ax1.set_xlabel('Frame')\n",
    "    ax1.set_ylabel('Latent dim 0')\n",
    "    ax1.set_title('Latent Dimension 0 Over Time')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    ax2 = fig.add_subplot(132)\n",
    "    for i, latents in enumerate(all_latents):\n",
    "        ax2.plot(latents[:, 1], label=f'Traj {i}')\n",
    "    ax2.set_xlabel('Frame')\n",
    "    ax2.set_ylabel('Latent dim 1')\n",
    "    ax2.set_title('Latent Dimension 1 Over Time')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # 2D projection of latent trajectory\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    for i, latents in enumerate(all_latents):\n",
    "        ax3.plot(latents[:, 0], latents[:, 1], 'o-', markersize=2, alpha=0.7, label=f'Traj {i}')\n",
    "        ax3.plot(latents[0, 0], latents[0, 1], 'go', markersize=8)  # Start\n",
    "        ax3.plot(latents[-1, 0], latents[-1, 1], 'ro', markersize=8)  # End\n",
    "    ax3.set_xlabel('Latent dim 0')\n",
    "    ax3.set_ylabel('Latent dim 1')\n",
    "    ax3.set_title('Latent Space Trajectory')\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_latent_space(model, val_data, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save model checkpoint\ncheckpoint = {\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'history': history,\n    'config': {\n        'latent_dim': LATENT_DIM,\n        'n_heads': N_HEADS,\n        'n_layers': N_LAYERS,\n        'min_context': MIN_CONTEXT,\n        'max_context': MAX_CONTEXT,\n        'predict_steps': PREDICT_STEPS,\n    }\n}\ntorch.save(checkpoint, 'results/model_checkpoint.pt')\nprint(\"Model saved to results/model_checkpoint.pt\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key metrics to track:\n",
    "- **Single-step MSE**: How well does model predict 1 frame ahead?\n",
    "- **Rollout degradation**: How fast does error grow with longer rollouts?\n",
    "- **Visual quality**: Do predictions look like valid physics?\n",
    "- **Latent structure**: Does latent space encode position/velocity?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}