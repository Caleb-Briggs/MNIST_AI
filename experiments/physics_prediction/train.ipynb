{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Video Prediction - Training\n",
    "\n",
    "Train a transformer-based model to predict physics simulation frames.\n",
    "\n",
    "**Architecture**: CNN Encoder → Transformer → CNN Decoder\n",
    "\n",
    "**Goal**: Model learns to compress video to latent state that captures physics (position, velocity, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo (for Colab)\n",
    "!git clone https://github.com/Caleb-Briggs/MNIST_AI.git\n",
    "%cd MNIST_AI/experiments/physics_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from physics_sim import (\n",
    "    Ball, Barrier, PhysicsSimulation,\n",
    "    generate_trajectory, create_random_simulation, generate_dataset\n",
    ")\n",
    "from model import VideoPredictor, count_parameters\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data config\n",
    "NUM_TRAJECTORIES = 500\n",
    "FRAMES_PER_TRAJECTORY = 100\n",
    "NUM_BARRIERS = 3\n",
    "WITH_GRAVITY = False  # Start simple\n",
    "\n",
    "# Model config\n",
    "LATENT_DIM = 256\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 4\n",
    "CONTEXT_LEN = 5  # Number of frames to condition on\n",
    "\n",
    "# Training config\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating training data...\")\n",
    "data = generate_dataset(\n",
    "    num_trajectories=NUM_TRAJECTORIES,\n",
    "    num_frames=FRAMES_PER_TRAJECTORY,\n",
    "    num_barriers=NUM_BARRIERS,\n",
    "    with_gravity=WITH_GRAVITY,\n",
    "    base_seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Memory: {data.nbytes / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Convert to torch tensor\n",
    "data_tensor = torch.from_numpy(data).float().unsqueeze(2)  # Add channel dim: (N, T, 1, H, W)\n",
    "print(f\"Tensor shape: {data_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few trajectories\n",
    "fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "for row in range(3):\n",
    "    for col in range(8):\n",
    "        t = col * 10\n",
    "        axes[row, col].imshow(data[row, t], cmap='gray', vmin=0, vmax=1)\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(f't={t}')\n",
    "        axes[row, col].axis('off')\n",
    "plt.suptitle('Sample Trajectories')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset that returns (context_frames, target_frame) pairs.\"\"\"\n",
    "    \n",
    "    def __init__(self, trajectories: torch.Tensor, context_len: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trajectories: (num_traj, num_frames, 1, H, W)\n",
    "            context_len: number of frames to use as context\n",
    "        \"\"\"\n",
    "        self.trajectories = trajectories\n",
    "        self.context_len = context_len\n",
    "        self.num_traj = trajectories.size(0)\n",
    "        self.num_frames = trajectories.size(1)\n",
    "        \n",
    "        # Each trajectory yields (num_frames - context_len) samples\n",
    "        self.samples_per_traj = self.num_frames - context_len\n",
    "        self.total_samples = self.num_traj * self.samples_per_traj\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        traj_idx = idx // self.samples_per_traj\n",
    "        frame_idx = idx % self.samples_per_traj\n",
    "        \n",
    "        # Context: frames [frame_idx : frame_idx + context_len]\n",
    "        # Target: frame [frame_idx + context_len]\n",
    "        context = self.trajectories[traj_idx, frame_idx:frame_idx + self.context_len]\n",
    "        target = self.trajectories[traj_idx, frame_idx + self.context_len]\n",
    "        \n",
    "        return context, target\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.9 * NUM_TRAJECTORIES)\n",
    "train_data = data_tensor[:train_size]\n",
    "val_data = data_tensor[train_size:]\n",
    "\n",
    "train_dataset = PhysicsDataset(train_data, CONTEXT_LEN)\n",
    "val_dataset = PhysicsDataset(val_data, CONTEXT_LEN)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoPredictor(\n",
    "    latent_dim=LATENT_DIM,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    dim_feedforward=LATENT_DIM * 2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "sample_context, sample_target = next(iter(train_loader))\n",
    "sample_context = sample_context.to(device)\n",
    "with torch.no_grad():\n",
    "    sample_pred = model(sample_context)\n",
    "print(f\"Input shape: {sample_context.shape}\")\n",
    "print(f\"Output shape: {sample_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "# Loss function: MSE on pixels\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'lr': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    for context, target in loader:\n        context = context.to(device)\n        target = target.to(device)\n        \n        optimizer.zero_grad()\n        pred = model(context)\n        loss = criterion(pred, target)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * context.size(0)\n    \n    return total_loss / len(loader.dataset)\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    for context, target in loader:\n        context = context.to(device)\n        target = target.to(device)\n        \n        pred = model(context)\n        loss = criterion(pred, target)\n        \n        total_loss += loss.item() * context.size(0)\n    \n    return total_loss / len(loader.dataset)\n\n@torch.no_grad()\ndef quick_visualize(model, val_data, device, epoch, context_len=5):\n    \"\"\"Quick visualization during training - shows a few predictions and a short rollout.\"\"\"\n    model.eval()\n    \n    fig, axes = plt.subplots(2, 6, figsize=(15, 5))\n    \n    # Row 0: Single-step prediction (3 examples)\n    for col in range(3):\n        idx = col * 10\n        context = val_data[0, idx:idx+context_len].unsqueeze(0).to(device)\n        target = val_data[0, idx+context_len]  # (1, 64, 64)\n        pred = model(context).cpu().squeeze(0)  # (1, 64, 64) - squeeze batch only\n        \n        # Show last context frame\n        axes[0, col*2].imshow(context[0, -1, 0].cpu(), cmap='gray', vmin=0, vmax=1)\n        axes[0, col*2].set_title(f'Input (t={idx+context_len-1})')\n        axes[0, col*2].axis('off')\n        \n        # Show prediction vs target overlay\n        axes[0, col*2+1].imshow(target[0].numpy(), cmap='gray', vmin=0, vmax=1, alpha=0.5)\n        axes[0, col*2+1].imshow(pred[0].clamp(0, 1).numpy(), cmap='Blues', vmin=0, vmax=1, alpha=0.5)\n        axes[0, col*2+1].set_title(f'Pred (blue) vs GT')\n        axes[0, col*2+1].axis('off')\n    \n    # Row 1: Short rollout\n    context = val_data[0, :context_len].unsqueeze(0).to(device)\n    rollout = model.rollout(context, 6).cpu().squeeze(0)  # (6, 1, 64, 64)\n    gt = val_data[0, context_len:context_len+6]  # (6, 1, 64, 64)\n    \n    for col in range(6):\n        axes[1, col].imshow(gt[col, 0].numpy(), cmap='gray', vmin=0, vmax=1, alpha=0.5)\n        axes[1, col].imshow(rollout[col, 0].clamp(0, 1).numpy(), cmap='Blues', vmin=0, vmax=1, alpha=0.5)\n        axes[1, col].set_title(f'Rollout t+{col+1}')\n        axes[1, col].axis('off')\n    \n    plt.suptitle(f'Epoch {epoch} - Blue=Predicted, Gray=Ground Truth')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualization frequency\nVIS_EVERY = 10  # Show visualizations every N epochs\n\nprint(f\"Training for {NUM_EPOCHS} epochs...\")\nprint(f\"Visualizations every {VIS_EVERY} epochs\")\nprint(\"=\"*60)\n\nfor epoch in range(NUM_EPOCHS):\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n    val_loss = eval_epoch(model, val_loader, criterion, device)\n    \n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['lr'].append(optimizer.param_groups[0]['lr'])\n    \n    scheduler.step()\n    \n    # Print progress\n    if (epoch + 1) % 5 == 0 or epoch == 0:\n        print(f\"Epoch {epoch+1:3d}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}\")\n    \n    # Periodic visualization\n    if (epoch + 1) % VIS_EVERY == 0 or epoch == 0:\n        quick_visualize(model, val_data, device, epoch + 1, CONTEXT_LEN)\n\nprint(\"=\"*60)\nprint(\"Training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(history['lr'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Step Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_predictions(model, dataset, device, num_samples=5):\n",
    "    \"\"\"Show context, prediction, ground truth, and error.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, CONTEXT_LEN + 3, figsize=(3*(CONTEXT_LEN+3), 3*num_samples))\n",
    "    \n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    for row, idx in enumerate(indices):\n",
    "        context, target = dataset[idx]\n",
    "        context = context.unsqueeze(0).to(device)  # Add batch dim\n",
    "        pred = model(context).cpu().squeeze()\n",
    "        target = target.squeeze()\n",
    "        context = context.cpu().squeeze()\n",
    "        \n",
    "        # Show context frames\n",
    "        for i in range(CONTEXT_LEN):\n",
    "            axes[row, i].imshow(context[i, 0], cmap='gray', vmin=0, vmax=1)\n",
    "            if row == 0:\n",
    "                axes[row, i].set_title(f'Context t-{CONTEXT_LEN-1-i}')\n",
    "            axes[row, i].axis('off')\n",
    "        \n",
    "        # Show prediction\n",
    "        axes[row, CONTEXT_LEN].imshow(pred[0], cmap='gray', vmin=0, vmax=1)\n",
    "        if row == 0:\n",
    "            axes[row, CONTEXT_LEN].set_title('Predicted')\n",
    "        axes[row, CONTEXT_LEN].axis('off')\n",
    "        \n",
    "        # Show ground truth\n",
    "        axes[row, CONTEXT_LEN+1].imshow(target[0], cmap='gray', vmin=0, vmax=1)\n",
    "        if row == 0:\n",
    "            axes[row, CONTEXT_LEN+1].set_title('Ground Truth')\n",
    "        axes[row, CONTEXT_LEN+1].axis('off')\n",
    "        \n",
    "        # Show error\n",
    "        error = torch.abs(pred[0] - target[0])\n",
    "        axes[row, CONTEXT_LEN+2].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n",
    "        if row == 0:\n",
    "            axes[row, CONTEXT_LEN+2].set_title('Error')\n",
    "        axes[row, CONTEXT_LEN+2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, val_dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_rollout(model, trajectories, device, traj_idx=0, rollout_steps=30):\n",
    "    \"\"\"Compare autoregressive rollout with ground truth.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get initial context\n",
    "    context = trajectories[traj_idx, :CONTEXT_LEN].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Ground truth future\n",
    "    gt_future = trajectories[traj_idx, CONTEXT_LEN:CONTEXT_LEN+rollout_steps].cpu().numpy()\n",
    "    \n",
    "    # Autoregressive rollout\n",
    "    predicted = model.rollout(context, rollout_steps).cpu().squeeze().numpy()\n",
    "    \n",
    "    # Visualize\n",
    "    num_show = min(10, rollout_steps)\n",
    "    step_indices = np.linspace(0, rollout_steps-1, num_show, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_show, figsize=(2*num_show, 6))\n",
    "    \n",
    "    for col, t in enumerate(step_indices):\n",
    "        # Predicted\n",
    "        axes[0, col].imshow(predicted[t, 0], cmap='gray', vmin=0, vmax=1)\n",
    "        if col == 0:\n",
    "            axes[0, col].set_ylabel('Predicted')\n",
    "        axes[0, col].set_title(f't+{t+1}')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[1, col].imshow(gt_future[t, 0], cmap='gray', vmin=0, vmax=1)\n",
    "        if col == 0:\n",
    "            axes[1, col].set_ylabel('Ground Truth')\n",
    "        axes[1, col].axis('off')\n",
    "        \n",
    "        # Error\n",
    "        error = np.abs(predicted[t, 0] - gt_future[t, 0])\n",
    "        axes[2, col].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n",
    "        if col == 0:\n",
    "            axes[2, col].set_ylabel('Error')\n",
    "        axes[2, col].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Autoregressive Rollout ({rollout_steps} steps)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute MSE over time\n",
    "    mse_over_time = np.mean((predicted[:, 0] - gt_future[:, 0])**2, axis=(1, 2))\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, rollout_steps+1), mse_over_time)\n",
    "    plt.xlabel('Rollout Step')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('Prediction Error vs Rollout Length')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Test on a few different trajectories\n",
    "for traj_idx in [0, 10, 20]:\n",
    "    print(f\"\\nTrajectory {traj_idx}:\")\n",
    "    visualize_rollout(model, val_data, device, traj_idx=traj_idx % len(val_data), rollout_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animated Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def animate_rollout_comparison(model, trajectories, device, traj_idx=0, rollout_steps=50):\n",
    "    \"\"\"Create side-by-side animation of prediction vs ground truth.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get data\n",
    "    context = trajectories[traj_idx, :CONTEXT_LEN].unsqueeze(0).to(device)\n",
    "    gt_future = trajectories[traj_idx, CONTEXT_LEN:CONTEXT_LEN+rollout_steps].cpu().numpy()\n",
    "    predicted = model.rollout(context, rollout_steps).cpu().squeeze().numpy()\n",
    "    \n",
    "    # Create animation\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    im_pred = axes[0].imshow(predicted[0, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0].set_title('Predicted')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    im_gt = axes[1].imshow(gt_future[0, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1].set_title('Ground Truth')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    error = np.abs(predicted[0, 0] - gt_future[0, 0])\n",
    "    im_err = axes[2].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n",
    "    axes[2].set_title('Error')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    title = fig.suptitle('t=0')\n",
    "    \n",
    "    def update(frame):\n",
    "        im_pred.set_array(predicted[frame, 0])\n",
    "        im_gt.set_array(gt_future[frame, 0])\n",
    "        error = np.abs(predicted[frame, 0] - gt_future[frame, 0])\n",
    "        im_err.set_array(error)\n",
    "        title.set_text(f't+{frame+1}')\n",
    "        return [im_pred, im_gt, im_err, title]\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, update, frames=rollout_steps, interval=100, blit=False)\n",
    "    plt.close()\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "animate_rollout_comparison(model, val_data, device, traj_idx=0, rollout_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis: Does the Model Learn Physics?\n",
    "\n",
    "### Latent Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def analyze_latent_space(model, trajectories, device, num_traj=5):\n",
    "    \"\"\"Analyze what the latent space encodes.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode several trajectories\n",
    "    all_latents = []\n",
    "    for i in range(num_traj):\n",
    "        traj = trajectories[i].unsqueeze(0).to(device)  # (1, T, 1, H, W)\n",
    "        latents = model.encode_frames(traj).cpu().numpy()  # (1, T, latent_dim)\n",
    "        all_latents.append(latents[0])\n",
    "    \n",
    "    # Plot latent trajectories (first 3 dimensions)\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    ax1 = fig.add_subplot(131)\n",
    "    for i, latents in enumerate(all_latents):\n",
    "        ax1.plot(latents[:, 0], label=f'Traj {i}')\n",
    "    ax1.set_xlabel('Frame')\n",
    "    ax1.set_ylabel('Latent dim 0')\n",
    "    ax1.set_title('Latent Dimension 0 Over Time')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    ax2 = fig.add_subplot(132)\n",
    "    for i, latents in enumerate(all_latents):\n",
    "        ax2.plot(latents[:, 1], label=f'Traj {i}')\n",
    "    ax2.set_xlabel('Frame')\n",
    "    ax2.set_ylabel('Latent dim 1')\n",
    "    ax2.set_title('Latent Dimension 1 Over Time')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # 2D projection of latent trajectory\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    for i, latents in enumerate(all_latents):\n",
    "        ax3.plot(latents[:, 0], latents[:, 1], 'o-', markersize=2, alpha=0.7, label=f'Traj {i}')\n",
    "        ax3.plot(latents[0, 0], latents[0, 1], 'go', markersize=8)  # Start\n",
    "        ax3.plot(latents[-1, 0], latents[-1, 1], 'ro', markersize=8)  # End\n",
    "    ax3.set_xlabel('Latent dim 0')\n",
    "    ax3.set_ylabel('Latent dim 1')\n",
    "    ax3.set_title('Latent Space Trajectory')\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_latent_space(model, val_data, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'history': history,\n",
    "    'config': {\n",
    "        'latent_dim': LATENT_DIM,\n",
    "        'n_heads': N_HEADS,\n",
    "        'n_layers': N_LAYERS,\n",
    "        'context_len': CONTEXT_LEN,\n",
    "    }\n",
    "}\n",
    "torch.save(checkpoint, 'results/model_checkpoint.pt')\n",
    "print(\"Model saved to results/model_checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key metrics to track:\n",
    "- **Single-step MSE**: How well does model predict 1 frame ahead?\n",
    "- **Rollout degradation**: How fast does error grow with longer rollouts?\n",
    "- **Visual quality**: Do predictions look like valid physics?\n",
    "- **Latent structure**: Does latent space encode position/velocity?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}