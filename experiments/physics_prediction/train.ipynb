{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Video Prediction - Training\n",
    "\n",
    "Train a transformer-based model to predict physics simulation frames.\n",
    "\n",
    "**Architecture**: CNN Encoder → Transformer → CNN Decoder\n",
    "\n",
    "**Goal**: Model learns to compress video to latent state that captures physics (position, velocity, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo (for Colab)\n",
    "!git clone https://github.com/Caleb-Briggs/MNIST_AI.git\n",
    "%cd MNIST_AI/experiments/physics_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from physics_sim import (\n",
    "    Ball, Barrier, PhysicsSimulation,\n",
    "    generate_trajectory, create_random_simulation, generate_dataset\n",
    ")\n",
    "from model import VideoPredictor, count_parameters\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data config\nNUM_TRAJECTORIES = 2000  # More trajectories since each terrain = one context window\nFRAMES_PER_TRAJECTORY = 50  # Shorter trajectories (context + rollout is enough)\nNUM_BARRIERS = 3\nWITH_GRAVITY = False  # Start simple\n\n# Model config\nLATENT_DIM = 256\nN_HEADS = 4\nN_LAYERS = 4\nCONTEXT_LEN = 8  # Fixed context window (not shifted during rollout)\nROLLOUT_STEPS = 10  # Predict this many frames from fixed context\n\n# Training config\nBATCH_SIZE = 128  # Larger batch for A100\nLEARNING_RATE = 3e-4  # Slightly higher LR for larger batch\nNUM_EPOCHS = 50\nSEED = 42\n\n# Derived\nMAX_ROLLOUT = FRAMES_PER_TRAJECTORY - CONTEXT_LEN  # Max frames we can predict\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\nprint(f\"Each trajectory: {CONTEXT_LEN} context frames -> up to {MAX_ROLLOUT} rollout frames\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating training data...\")\n",
    "data = generate_dataset(\n",
    "    num_trajectories=NUM_TRAJECTORIES,\n",
    "    num_frames=FRAMES_PER_TRAJECTORY,\n",
    "    num_barriers=NUM_BARRIERS,\n",
    "    with_gravity=WITH_GRAVITY,\n",
    "    base_seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Memory: {data.nbytes / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Convert to torch tensor\n",
    "data_tensor = torch.from_numpy(data).float().unsqueeze(2)  # Add channel dim: (N, T, 1, H, W)\n",
    "print(f\"Tensor shape: {data_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few trajectories\n",
    "fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "for row in range(3):\n",
    "    for col in range(8):\n",
    "        t = col * 10\n",
    "        axes[row, col].imshow(data[row, t], cmap='gray', vmin=0, vmax=1)\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(f't={t}')\n",
    "        axes[row, col].axis('off')\n",
    "plt.suptitle('Sample Trajectories')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class PhysicsDataset(torch.utils.data.Dataset):\n    \"\"\"Dataset that returns (context_frames, all_future_frames) per trajectory.\n    \n    Each trajectory = one terrain configuration = one training sample.\n    Context window is FIXED (not shifted during rollout).\n    \"\"\"\n    \n    def __init__(self, trajectories: torch.Tensor, context_len: int):\n        \"\"\"\n        Args:\n            trajectories: (num_traj, num_frames, 1, H, W)\n            context_len: number of frames to use as fixed context\n        \"\"\"\n        self.trajectories = trajectories\n        self.context_len = context_len\n        self.num_traj = trajectories.size(0)\n        self.num_frames = trajectories.size(1)\n    \n    def __len__(self):\n        return self.num_traj\n    \n    def __getitem__(self, idx):\n        # Context: first context_len frames (FIXED, never shifts)\n        # Targets: ALL remaining frames\n        context = self.trajectories[idx, :self.context_len]  # (context_len, 1, 64, 64)\n        targets = self.trajectories[idx, self.context_len:]  # (num_frames - context_len, 1, 64, 64)\n        \n        return context, targets\n\n# Split into train/val\ntrain_size = int(0.9 * NUM_TRAJECTORIES)\ntrain_data = data_tensor[:train_size]\nval_data = data_tensor[train_size:]\n\ntrain_dataset = PhysicsDataset(train_data, CONTEXT_LEN)\nval_dataset = PhysicsDataset(val_data, CONTEXT_LEN)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True\n)\nval_loader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True\n)\n\nprint(f\"Train trajectories: {len(train_dataset)}\")\nprint(f\"Val trajectories: {len(val_dataset)}\")\nprint(f\"Each sample: {CONTEXT_LEN} fixed context frames -> {MAX_ROLLOUT} target frames\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoPredictor(\n",
    "    latent_dim=LATENT_DIM,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    dim_feedforward=LATENT_DIM * 2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "sample_context, sample_target = next(iter(train_loader))\n",
    "sample_context = sample_context.to(device)\n",
    "with torch.no_grad():\n",
    "    sample_pred = model(sample_context)\n",
    "print(f\"Input shape: {sample_context.shape}\")\n",
    "print(f\"Output shape: {sample_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "# Loss function: MSE on pixels\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'lr': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(model, loader, optimizer, criterion, device, rollout_steps):\n    \"\"\"Train with parallel multi-step prediction (fixed context).\"\"\"\n    model.train()\n    total_loss = 0\n    \n    for context, targets in loader:\n        context = context.to(device)  # (batch, context_len, 1, 64, 64)\n        targets = targets.to(device)  # (batch, max_rollout, 1, 64, 64)\n        \n        optimizer.zero_grad()\n        \n        # Predict all future frames in parallel from fixed context\n        # Only use first rollout_steps targets for training\n        preds = model(context, n_future=rollout_steps)  # (batch, rollout_steps, 1, 64, 64)\n        targets_subset = targets[:, :rollout_steps]  # (batch, rollout_steps, 1, 64, 64)\n        \n        # MSE loss over all predicted frames\n        loss = criterion(preds, targets_subset)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * context.size(0)\n    \n    return total_loss / len(loader.dataset)\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion, device, rollout_steps):\n    \"\"\"Evaluate with parallel multi-step prediction.\"\"\"\n    model.eval()\n    total_loss = 0\n    \n    for context, targets in loader:\n        context = context.to(device)\n        targets = targets.to(device)\n        \n        preds = model(context, n_future=rollout_steps)\n        targets_subset = targets[:, :rollout_steps]\n        \n        loss = criterion(preds, targets_subset)\n        total_loss += loss.item() * context.size(0)\n    \n    return total_loss / len(loader.dataset)\n\n@torch.no_grad()\ndef quick_visualize(model, val_data, device, epoch, context_len=8):\n    \"\"\"Quick visualization during training - shows predictions and rollout.\"\"\"\n    model.eval()\n    \n    fig, axes = plt.subplots(2, 6, figsize=(15, 5))\n    \n    # Row 0: Single-step predictions (3 examples from different trajectories)\n    for col in range(3):\n        traj_idx = col * 10\n        context = val_data[traj_idx, :context_len].unsqueeze(0).to(device)\n        target = val_data[traj_idx, context_len]  # (1, 64, 64)\n        pred = model.predict_next(context).cpu().squeeze(0)  # (1, 64, 64)\n        \n        # Show last context frame\n        axes[0, col*2].imshow(context[0, -1, 0].cpu(), cmap='gray', vmin=0, vmax=1)\n        axes[0, col*2].set_title(f'Context (traj {traj_idx})')\n        axes[0, col*2].axis('off')\n        \n        # Show prediction vs target overlay\n        axes[0, col*2+1].imshow(target[0].numpy(), cmap='gray', vmin=0, vmax=1, alpha=0.5)\n        axes[0, col*2+1].imshow(pred[0].clamp(0, 1).numpy(), cmap='Blues', vmin=0, vmax=1, alpha=0.5)\n        axes[0, col*2+1].set_title(f'Pred vs GT')\n        axes[0, col*2+1].axis('off')\n    \n    # Row 1: Longer rollout from first trajectory\n    context = val_data[0, :context_len].unsqueeze(0).to(device)\n    rollout = model.rollout(context, 6).cpu().squeeze(0)  # (6, 1, 64, 64)\n    gt = val_data[0, context_len:context_len+6]  # (6, 1, 64, 64)\n    \n    for col in range(6):\n        axes[1, col].imshow(gt[col, 0].numpy(), cmap='gray', vmin=0, vmax=1, alpha=0.5)\n        axes[1, col].imshow(rollout[col, 0].clamp(0, 1).numpy(), cmap='Blues', vmin=0, vmax=1, alpha=0.5)\n        axes[1, col].set_title(f't+{col+1}')\n        axes[1, col].axis('off')\n    \n    plt.suptitle(f'Epoch {epoch} - Blue=Predicted, Gray=Ground Truth')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualization frequency\nVIS_EVERY = 10  # Show visualizations every N epochs\n\nprint(f\"Training for {NUM_EPOCHS} epochs...\")\nprint(f\"Multi-step rollout training: {ROLLOUT_STEPS} steps\")\nprint(f\"Visualizations every {VIS_EVERY} epochs\")\nprint(\"=\"*60)\n\nfor epoch in range(NUM_EPOCHS):\n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device, ROLLOUT_STEPS)\n    val_loss = eval_epoch(model, val_loader, criterion, device, ROLLOUT_STEPS)\n    \n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['lr'].append(optimizer.param_groups[0]['lr'])\n    \n    scheduler.step()\n    \n    # Print progress\n    if (epoch + 1) % 5 == 0 or epoch == 0:\n        print(f\"Epoch {epoch+1:3d}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}\")\n    \n    # Periodic visualization\n    if (epoch + 1) % VIS_EVERY == 0 or epoch == 0:\n        quick_visualize(model, val_data, device, epoch + 1, CONTEXT_LEN)\n\nprint(\"=\"*60)\nprint(\"Training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(history['lr'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Step Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@torch.no_grad()\ndef visualize_predictions(model, dataset, device, num_samples=5, num_future=3):\n    \"\"\"Show context, predictions at different horizons, and ground truth.\"\"\"\n    model.eval()\n    \n    # Columns: context frames + predicted frames + ground truth frames\n    num_cols = CONTEXT_LEN + num_future * 2\n    fig, axes = plt.subplots(num_samples, num_cols, figsize=(2*num_cols, 2.5*num_samples))\n    \n    indices = np.random.choice(len(dataset), num_samples, replace=False)\n    \n    for row, idx in enumerate(indices):\n        context, targets = dataset[idx]\n        context = context.unsqueeze(0).to(device)  # Add batch dim\n        preds = model(context, n_future=num_future).cpu().squeeze(0)  # (num_future, 1, 64, 64)\n        targets = targets[:num_future]  # (num_future, 1, 64, 64)\n        context = context.cpu().squeeze(0)\n        \n        col = 0\n        # Show context frames\n        for i in range(CONTEXT_LEN):\n            axes[row, col].imshow(context[i, 0], cmap='gray', vmin=0, vmax=1)\n            if row == 0:\n                axes[row, col].set_title(f'Ctx {i+1}')\n            axes[row, col].axis('off')\n            col += 1\n        \n        # Show predicted and ground truth frames side by side\n        for t in range(num_future):\n            # Predicted\n            axes[row, col].imshow(preds[t, 0].clamp(0, 1), cmap='gray', vmin=0, vmax=1)\n            if row == 0:\n                axes[row, col].set_title(f'Pred t+{t+1}')\n            axes[row, col].axis('off')\n            col += 1\n            \n            # Ground truth\n            axes[row, col].imshow(targets[t, 0], cmap='gray', vmin=0, vmax=1)\n            if row == 0:\n                axes[row, col].set_title(f'GT t+{t+1}')\n            axes[row, col].axis('off')\n            col += 1\n    \n    plt.suptitle('Context → Predictions vs Ground Truth')\n    plt.tight_layout()\n    plt.show()\n\nvisualize_predictions(model, val_dataset, device, num_samples=5, num_future=5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_rollout(model, trajectories, device, traj_idx=0, rollout_steps=30):\n",
    "    \"\"\"Compare autoregressive rollout with ground truth.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get initial context\n",
    "    context = trajectories[traj_idx, :CONTEXT_LEN].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Ground truth future\n",
    "    gt_future = trajectories[traj_idx, CONTEXT_LEN:CONTEXT_LEN+rollout_steps].cpu().numpy()\n",
    "    \n",
    "    # Autoregressive rollout\n",
    "    predicted = model.rollout(context, rollout_steps).cpu().squeeze().numpy()\n",
    "    \n",
    "    # Visualize\n",
    "    num_show = min(10, rollout_steps)\n",
    "    step_indices = np.linspace(0, rollout_steps-1, num_show, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_show, figsize=(2*num_show, 6))\n",
    "    \n",
    "    for col, t in enumerate(step_indices):\n",
    "        # Predicted\n",
    "        axes[0, col].imshow(predicted[t, 0], cmap='gray', vmin=0, vmax=1)\n",
    "        if col == 0:\n",
    "            axes[0, col].set_ylabel('Predicted')\n",
    "        axes[0, col].set_title(f't+{t+1}')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[1, col].imshow(gt_future[t, 0], cmap='gray', vmin=0, vmax=1)\n",
    "        if col == 0:\n",
    "            axes[1, col].set_ylabel('Ground Truth')\n",
    "        axes[1, col].axis('off')\n",
    "        \n",
    "        # Error\n",
    "        error = np.abs(predicted[t, 0] - gt_future[t, 0])\n",
    "        axes[2, col].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n",
    "        if col == 0:\n",
    "            axes[2, col].set_ylabel('Error')\n",
    "        axes[2, col].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Autoregressive Rollout ({rollout_steps} steps)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute MSE over time\n",
    "    mse_over_time = np.mean((predicted[:, 0] - gt_future[:, 0])**2, axis=(1, 2))\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, rollout_steps+1), mse_over_time)\n",
    "    plt.xlabel('Rollout Step')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('Prediction Error vs Rollout Length')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Test on a few different trajectories\n",
    "for traj_idx in [0, 10, 20]:\n",
    "    print(f\"\\nTrajectory {traj_idx}:\")\n",
    "    visualize_rollout(model, val_data, device, traj_idx=traj_idx % len(val_data), rollout_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animated Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def animate_rollout_comparison(model, trajectories, device, traj_idx=0, rollout_steps=50):\n",
    "    \"\"\"Create side-by-side animation of prediction vs ground truth.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get data\n",
    "    context = trajectories[traj_idx, :CONTEXT_LEN].unsqueeze(0).to(device)\n",
    "    gt_future = trajectories[traj_idx, CONTEXT_LEN:CONTEXT_LEN+rollout_steps].cpu().numpy()\n",
    "    predicted = model.rollout(context, rollout_steps).cpu().squeeze().numpy()\n",
    "    \n",
    "    # Create animation\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    im_pred = axes[0].imshow(predicted[0, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0].set_title('Predicted')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    im_gt = axes[1].imshow(gt_future[0, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1].set_title('Ground Truth')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    error = np.abs(predicted[0, 0] - gt_future[0, 0])\n",
    "    im_err = axes[2].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n",
    "    axes[2].set_title('Error')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    title = fig.suptitle('t=0')\n",
    "    \n",
    "    def update(frame):\n",
    "        im_pred.set_array(predicted[frame, 0])\n",
    "        im_gt.set_array(gt_future[frame, 0])\n",
    "        error = np.abs(predicted[frame, 0] - gt_future[frame, 0])\n",
    "        im_err.set_array(error)\n",
    "        title.set_text(f't+{frame+1}')\n",
    "        return [im_pred, im_gt, im_err, title]\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, update, frames=rollout_steps, interval=100, blit=False)\n",
    "    plt.close()\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "animate_rollout_comparison(model, val_data, device, traj_idx=0, rollout_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis: Does the Model Learn Physics?\n",
    "\n",
    "### Latent Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def analyze_latent_space(model, trajectories, device, num_traj=5):\n",
    "    \"\"\"Analyze what the latent space encodes.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode several trajectories\n",
    "    all_latents = []\n",
    "    for i in range(num_traj):\n",
    "        traj = trajectories[i].unsqueeze(0).to(device)  # (1, T, 1, H, W)\n",
    "        latents = model.encode_frames(traj).cpu().numpy()  # (1, T, latent_dim)\n",
    "        all_latents.append(latents[0])\n",
    "    \n",
    "    # Plot latent trajectories (first 3 dimensions)\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    ax1 = fig.add_subplot(131)\n",
    "    for i, latents in enumerate(all_latents):\n",
    "        ax1.plot(latents[:, 0], label=f'Traj {i}')\n",
    "    ax1.set_xlabel('Frame')\n",
    "    ax1.set_ylabel('Latent dim 0')\n",
    "    ax1.set_title('Latent Dimension 0 Over Time')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    ax2 = fig.add_subplot(132)\n",
    "    for i, latents in enumerate(all_latents):\n",
    "        ax2.plot(latents[:, 1], label=f'Traj {i}')\n",
    "    ax2.set_xlabel('Frame')\n",
    "    ax2.set_ylabel('Latent dim 1')\n",
    "    ax2.set_title('Latent Dimension 1 Over Time')\n",
    "    ax2.grid(alpha=0.3)\n",
    "    \n",
    "    # 2D projection of latent trajectory\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    for i, latents in enumerate(all_latents):\n",
    "        ax3.plot(latents[:, 0], latents[:, 1], 'o-', markersize=2, alpha=0.7, label=f'Traj {i}')\n",
    "        ax3.plot(latents[0, 0], latents[0, 1], 'go', markersize=8)  # Start\n",
    "        ax3.plot(latents[-1, 0], latents[-1, 1], 'ro', markersize=8)  # End\n",
    "    ax3.set_xlabel('Latent dim 0')\n",
    "    ax3.set_ylabel('Latent dim 1')\n",
    "    ax3.set_title('Latent Space Trajectory')\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_latent_space(model, val_data, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'history': history,\n",
    "    'config': {\n",
    "        'latent_dim': LATENT_DIM,\n",
    "        'n_heads': N_HEADS,\n",
    "        'n_layers': N_LAYERS,\n",
    "        'context_len': CONTEXT_LEN,\n",
    "    }\n",
    "}\n",
    "torch.save(checkpoint, 'results/model_checkpoint.pt')\n",
    "print(\"Model saved to results/model_checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key metrics to track:\n",
    "- **Single-step MSE**: How well does model predict 1 frame ahead?\n",
    "- **Rollout degradation**: How fast does error grow with longer rollouts?\n",
    "- **Visual quality**: Do predictions look like valid physics?\n",
    "- **Latent structure**: Does latent space encode position/velocity?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}