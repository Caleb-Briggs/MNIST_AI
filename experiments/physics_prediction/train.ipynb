{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Video Prediction - Training\n",
    "\n",
    "Train a modern transformer to predict physics simulation frames.\n",
    "\n",
    "**Architecture**: LLaMA-style transformer (RMSNorm, SwiGLU, RoPE)\n",
    "- Each 32x32 frame is flattened to a 1024-dim vector (one \"token\" per frame)\n",
    "- Transformer predicts next frame from variable-length context\n",
    "- Always starts from frame 1 (teacher forcing)\n",
    "\n",
    "**Goal**: Model learns physics (position, velocity) from raw pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MNIST_AI'...\n",
      "remote: Enumerating objects: 170, done.\u001b[K\n",
      "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
      "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
      "remote: Total 170 (delta 60), reused 62 (delta 24), pack-reused 72 (from 1)\u001b[K\n",
      "Receiving objects: 100% (170/170), 22.98 MiB | 27.65 MiB/s, done.B/s\n",
      "Resolving deltas: 100% (79/79), done.\n",
      "/content/MNIST_AI/experiments/physics_prediction\n",
      "Setup complete! Checkpoints save to results/\n"
     ]
    }
   ],
   "source": [
    "# === COLAB SETUP ===\n",
    "# Clone repo (run once)\n",
    "!rm -rf MNIST_AI 2>/dev/null\n",
    "!git clone https://github.com/Caleb-Briggs/MNIST_AI.git\n",
    "%cd MNIST_AI/experiments/physics_prediction\n",
    "\n",
    "# Create results directory\n",
    "!mkdir -p results\n",
    "\n",
    "# === STORAGE OPTIONS ===\n",
    "# Option 1: Google Drive (browser-based Colab only)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !mkdir -p /content/drive/MyDrive/physics_prediction_results\n",
    "# !ln -sf /content/drive/MyDrive/physics_prediction_results results\n",
    "\n",
    "# Option 2: For VS Code - checkpoints save locally to results/\n",
    "# Download them manually or use the sync cell below\n",
    "\n",
    "print(\"Setup complete! Checkpoints save to results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "from physics_sim import (\n",
    "    Ball, Barrier, PhysicsSimulation,\n",
    "    generate_trajectory, create_random_simulation, generate_dataset\n",
    ")\n",
    "from model import VideoPredictor, count_parameters\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data config\n",
    "NUM_TRAJECTORIES = 5000\n",
    "FRAMES_PER_TRAJECTORY = 25  # Shorter trajectories, max context ~24\n",
    "NUM_BARRIERS = 2\n",
    "WITH_GRAVITY = False\n",
    "RESOLUTION = 32  # 32x32 frames\n",
    "DT = 6.0  # Time step - high enough that ball moves visibly\n",
    "\n",
    "# Model config\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 6\n",
    "DROPOUT = 0.0\n",
    "\n",
    "# Training config\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_EPOCHS = 1000\n",
    "SEED = 421\n",
    "\n",
    "# Variable context: sample context lengths from this range during training\n",
    "MIN_CONTEXT = 3   # At least 3 frames to see motion\n",
    "MAX_CONTEXT = 20  # Up to 20 frames of context\n",
    "\n",
    "# Data diversity: regenerate trajectories every N epochs\n",
    "REGEN_DATA_EVERY = 25\n",
    "\n",
    "# Visualization\n",
    "VIS_EVERY = 25\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Enable TF32 for faster matmuls on A100\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "print(f\"Model: {N_LAYERS} layers, {N_HEADS} heads, frame_dim=1024\")\n",
    "print(f\"Training: batch={BATCH_SIZE}, lr={LEARNING_RATE}, epochs={NUM_EPOCHS}\")\n",
    "print(f\"Context length: {MIN_CONTEXT}-{MAX_CONTEXT} frames\")\n",
    "print(f\"Data regeneration every {REGEN_DATA_EVERY} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating training data...\")\n",
    "data = generate_dataset(\n",
    "    num_trajectories=NUM_TRAJECTORIES,\n",
    "    num_frames=FRAMES_PER_TRAJECTORY,\n",
    "    num_barriers=NUM_BARRIERS,\n",
    "    with_gravity=WITH_GRAVITY,\n",
    "    resolution=RESOLUTION,\n",
    "    base_seed=SEED,\n",
    "    dt=DT\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Memory: {data.nbytes / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Convert to torch tensor (no channel dim needed now)\n",
    "data_tensor = torch.from_numpy(data).float()  # (N, T, H, W)\n",
    "print(f\"Tensor shape: {data_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few trajectories\n",
    "fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "for row in range(3):\n",
    "    for col in range(8):\n",
    "        t = col * 3  # 0, 3, 6, 9, 12, 15, 18, 21\n",
    "        axes[row, col].imshow(data[row, t], cmap='gray', vmin=0, vmax=1)\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(f't={t}')\n",
    "        axes[row, col].axis('off')\n",
    "plt.suptitle('Sample Trajectories (32x32)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class PhysicsDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset that samples (context, target) pairs from trajectories.\n    \n    Each sample:\n    - context: frames 0 to context_len-1 (always starts from frame 0)\n    - target: frame at context_len\n    \n    Context length is sampled randomly per-item, but we use a fixed\n    max context and simply use the actual frames (no padding needed\n    since we batch samples with the same context length).\n    \"\"\"\n    \n    def __init__(self, trajectories: torch.Tensor, min_context: int, max_context: int):\n        self.trajectories = trajectories  # (N, T, H, W)\n        self.min_context = min_context\n        self.max_context = min(max_context, trajectories.size(1) - 1)\n        self.num_traj = trajectories.size(0)\n        self.context_range = self.max_context - self.min_context + 1\n        \n    def __len__(self):\n        # Each trajectory gives us (max_context - min_context + 1) samples\n        return self.num_traj * self.context_range\n    \n    def __getitem__(self, idx):\n        traj_idx = idx // self.context_range\n        context_idx = idx % self.context_range\n        context_len = self.min_context + context_idx\n        \n        # Always start from frame 0, get context_len frames\n        context = self.trajectories[traj_idx, :context_len]  # (context_len, H, W)\n        target = self.trajectories[traj_idx, context_len]     # (H, W)\n        \n        return context, target, context_len\n\n\ndef collate_by_context_length(batch):\n    \"\"\"\n    Group samples by context length and pad to max in batch.\n    Returns contexts, targets, and a mask for valid positions.\n    \"\"\"\n    contexts, targets, lengths = zip(*batch)\n    \n    max_len = max(lengths)\n    batch_size = len(contexts)\n    H, W = contexts[0].shape[1], contexts[0].shape[2]\n    \n    # Create padded tensor and attention mask\n    padded_contexts = torch.zeros(batch_size, max_len, H, W)\n    # Mask: True = ignore this position (will be masked in attention)\n    attention_mask = torch.ones(batch_size, max_len, dtype=torch.bool)\n    \n    for i, (ctx, length) in enumerate(zip(contexts, lengths)):\n        # Put actual frames at the END (right-aligned) so causal attention works\n        start_idx = max_len - length\n        padded_contexts[i, start_idx:] = ctx\n        attention_mask[i, start_idx:] = False  # False = attend to this position\n    \n    targets_tensor = torch.stack(targets)\n    \n    return padded_contexts, targets_tensor, attention_mask\n\n\n# Split into train/val\ntrain_size = int(0.9 * NUM_TRAJECTORIES)\ntrain_data = data_tensor[:train_size]\nval_data = data_tensor[train_size:]\n\ntrain_dataset = PhysicsDataset(train_data, MIN_CONTEXT, MAX_CONTEXT)\nval_dataset = PhysicsDataset(val_data, MIN_CONTEXT, MAX_CONTEXT)\n\n# Dataloaders with custom collate function\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    num_workers=4,\n    pin_memory=True,\n    collate_fn=collate_by_context_length\n)\nval_loader = torch.utils.data.DataLoader(\n    val_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=False,\n    num_workers=4,\n    pin_memory=True,\n    collate_fn=collate_by_context_length\n)\n\nprint(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Val samples: {len(val_dataset)}\")\nprint(f\"Batches per epoch: {len(train_loader)}\")\n\n# Verify no NaN in data\nprint(f\"\\nData check:\")\nprint(f\"  Train data has NaN: {torch.isnan(train_data).any().item()}\")\nprint(f\"  Train data range: [{train_data.min():.3f}, {train_data.max():.3f}]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VideoPredictor(\n",
    "    frame_size=RESOLUTION,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Test forward pass with variable context\n",
    "sample_context, sample_target = next(iter(train_loader))\n",
    "sample_context = sample_context.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_pred = model(sample_context)\n",
    "\n",
    "print(f\"Context shape: {sample_context.shape}\")\n",
    "print(f\"Target shape: {sample_target.shape}\")\n",
    "print(f\"Prediction shape: {sample_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-5)\n",
    "\n",
    "# Loss function: MSE on pixels\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'lr': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mixed precision training\nscaler = torch.amp.GradScaler('cuda')\n\ndef train_epoch(model, loader, optimizer, criterion, device, scaler):\n    \"\"\"Train for one epoch with mixed precision.\"\"\"\n    model.train()\n    total_loss = 0\n    num_batches = 0\n    \n    for context, target, mask in loader:\n        context = context.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True)\n        mask = mask.to(device, non_blocking=True)\n        \n        optimizer.zero_grad(set_to_none=True)\n        \n        with torch.amp.autocast('cuda'):\n            pred = model(context, padding_mask=mask)\n            loss = criterion(pred, target)\n        \n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item()\n        num_batches += 1\n    \n    return total_loss / num_batches\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion, device):\n    \"\"\"Evaluate for one epoch.\"\"\"\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    \n    for context, target, mask in loader:\n        context = context.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True)\n        mask = mask.to(device, non_blocking=True)\n        \n        with torch.amp.autocast('cuda'):\n            pred = model(context, padding_mask=mask)\n            loss = criterion(pred, target)\n        \n        total_loss += loss.item()\n        num_batches += 1\n    \n    return total_loss / num_batches\n\n\n@torch.no_grad()\ndef quick_visualize(model, val_data, device, epoch):\n    \"\"\"Quick visualization during training.\"\"\"\n    model.eval()\n    \n    fig, axes = plt.subplots(3, 6, figsize=(14, 7))\n    \n    for row in range(3):\n        traj_idx = row * 50\n        context_len = 5 + row * 5  # 5, 10, 15 frames of context\n        \n        context = val_data[traj_idx, :context_len].unsqueeze(0).to(device)\n        target = val_data[traj_idx, context_len]\n        pred = model(context).cpu().squeeze(0)  # No padding mask needed for single sample\n        \n        # Show last 3 context frames\n        for i in range(3):\n            axes[row, i].imshow(context[0, -(3-i), :, :].cpu(), cmap='gray', vmin=0, vmax=1)\n            if row == 0:\n                axes[row, i].set_title(f'Ctx t-{2-i}')\n            axes[row, i].axis('off')\n        \n        # Prediction\n        axes[row, 3].imshow(pred.clamp(0, 1), cmap='gray', vmin=0, vmax=1)\n        if row == 0:\n            axes[row, 3].set_title('Predicted')\n        axes[row, 3].axis('off')\n        \n        # Ground truth\n        axes[row, 4].imshow(target, cmap='gray', vmin=0, vmax=1)\n        if row == 0:\n            axes[row, 4].set_title('Ground Truth')\n        axes[row, 4].axis('off')\n        \n        # Error\n        error = torch.abs(pred - target)\n        axes[row, 5].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n        if row == 0:\n            axes[row, 5].set_title('Error')\n        axes[row, 5].axis('off')\n        \n        # Label row with context length\n        axes[row, 0].set_ylabel(f'ctx={context_len}', fontsize=10)\n    \n    plt.suptitle(f'Epoch {epoch}')\n    plt.tight_layout()\n\n\ndef generate_fresh_data(seed_offset):\n    \"\"\"Generate fresh training data with new terrains.\"\"\"\n    print(f\"  Generating {NUM_TRAJECTORIES} fresh trajectories...\")\n    data = generate_dataset(\n        num_trajectories=NUM_TRAJECTORIES,\n        num_frames=FRAMES_PER_TRAJECTORY,\n        num_barriers=NUM_BARRIERS,\n        with_gravity=WITH_GRAVITY,\n        resolution=RESOLUTION,\n        base_seed=seed_offset,\n        dt=DT\n    )\n    return torch.from_numpy(data).float()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Main training loop\nprint(f\"Starting training for {NUM_EPOCHS} epochs...\")\nprint(f\"Data regeneration every {REGEN_DATA_EVERY} epochs\")\nprint(f\"Visualizations every {VIS_EVERY} epochs\")\nprint(\"=\"*60)\n\nbest_val_loss = float('inf')\ndata_seed_offset = SEED\n\n# Current data references\ncurrent_train_data = train_data\ncurrent_val_data = val_data\ncurrent_train_loader = train_loader\ncurrent_val_loader = val_loader\n\nfor epoch in range(NUM_EPOCHS):\n    # Regenerate data periodically\n    if epoch > 0 and epoch % REGEN_DATA_EVERY == 0:\n        print(f\"\\n{'='*60}\")\n        print(f\"Regenerating training data at epoch {epoch}\")\n        data_seed_offset += NUM_TRAJECTORIES\n        \n        fresh_data = generate_fresh_data(data_seed_offset)\n        \n        train_size = int(0.9 * NUM_TRAJECTORIES)\n        current_train_data = fresh_data[:train_size]\n        current_val_data = fresh_data[train_size:]\n        \n        current_train_loader = torch.utils.data.DataLoader(\n            PhysicsDataset(current_train_data, MIN_CONTEXT, MAX_CONTEXT),\n            batch_size=BATCH_SIZE, shuffle=True, num_workers=4,\n            pin_memory=True, collate_fn=collate_by_context_length\n        )\n        current_val_loader = torch.utils.data.DataLoader(\n            PhysicsDataset(current_val_data, MIN_CONTEXT, MAX_CONTEXT),\n            batch_size=BATCH_SIZE, shuffle=False, num_workers=4,\n            pin_memory=True, collate_fn=collate_by_context_length\n        )\n        print(f\"{'='*60}\\n\")\n    \n    # Train and evaluate\n    train_loss = train_epoch(model, current_train_loader, optimizer, criterion, device, scaler)\n    val_loss = eval_epoch(model, current_val_loader, criterion, device)\n    \n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['lr'].append(optimizer.param_groups[0]['lr'])\n    \n    scheduler.step()\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n    \n    lr = optimizer.param_groups[0]['lr']\n    print(f\"Epoch {epoch+1:4d}/{NUM_EPOCHS}: train={train_loss:.6f}, val={val_loss:.6f}, best={best_val_loss:.6f}, lr={lr:.2e}\")\n    \n    # Periodic visualization\n    if (epoch + 1) % VIS_EVERY == 0:\n        print(f\"\\n{'='*60}\")\n        print(f\"Visualization at epoch {epoch+1}\")\n        print(f\"{'='*60}\\n\")\n        \n        quick_visualize(model, current_val_data, device, epoch + 1)\n        plt.show()\n        \n        # Show rollout\n        print(\"\\nRollout comparison (8 steps):\")\n        model.eval()\n        with torch.no_grad():\n            traj_idx = 0\n            context = current_val_data[traj_idx, :5].unsqueeze(0).to(device)\n            gt_future = current_val_data[traj_idx, 5:13].cpu().numpy()\n            predicted = model.rollout(context, 8).cpu().squeeze(0).numpy()\n        \n        fig, axes = plt.subplots(2, 8, figsize=(14, 3.5))\n        for t in range(8):\n            axes[0, t].imshow(np.clip(predicted[t], 0, 1), cmap='gray', vmin=0, vmax=1)\n            axes[0, t].axis('off')\n            if t == 0:\n                axes[0, t].set_ylabel('Pred', fontsize=10)\n            axes[0, t].set_title(f't+{t+1}', fontsize=8)\n            \n            axes[1, t].imshow(gt_future[t], cmap='gray', vmin=0, vmax=1)\n            axes[1, t].axis('off')\n            if t == 0:\n                axes[1, t].set_ylabel('GT', fontsize=10)\n        \n        plt.suptitle(f'Rollout at Epoch {epoch+1}')\n        plt.tight_layout()\n        plt.show()\n        \n        # Loss curve\n        if len(history['train_loss']) > 1:\n            fig, ax = plt.subplots(figsize=(10, 3))\n            ax.plot(history['train_loss'], label='Train', alpha=0.8)\n            ax.plot(history['val_loss'], label='Val', alpha=0.8)\n            ax.set_xlabel('Epoch')\n            ax.set_ylabel('MSE Loss')\n            ax.set_title('Training Progress')\n            ax.legend()\n            ax.grid(alpha=0.3)\n            ax.set_yscale('log')\n            plt.tight_layout()\n            plt.show()\n        \n        print(f\"\\n{'='*60}\\n\")\n\nprint(f\"\\nTraining complete! Best val_loss: {best_val_loss:.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "\n",
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[1].plot(history['lr'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedule')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Step Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_predictions(model, val_data, device, num_samples=5):\n",
    "    \"\"\"Show context, prediction, ground truth, and error.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 6, figsize=(14, 2.5*num_samples))\n",
    "    \n",
    "    indices = np.random.choice(len(val_data), num_samples, replace=False)\n",
    "    \n",
    "    for row, traj_idx in enumerate(indices):\n",
    "        context_len = np.random.randint(MIN_CONTEXT, MAX_CONTEXT)\n",
    "        \n",
    "        context = val_data[traj_idx, :context_len].unsqueeze(0).to(device)\n",
    "        target = val_data[traj_idx, context_len]\n",
    "        pred = model(context).cpu().squeeze(0)\n",
    "        \n",
    "        # Show last 3 context frames\n",
    "        for i in range(3):\n",
    "            axes[row, i].imshow(context[0, -(3-i)].cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "            if row == 0:\n",
    "                axes[row, i].set_title(f'Ctx t-{2-i}')\n",
    "            axes[row, i].axis('off')\n",
    "        \n",
    "        axes[row, 3].imshow(pred.clamp(0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "        if row == 0:\n",
    "            axes[row, 3].set_title('Predicted')\n",
    "        axes[row, 3].axis('off')\n",
    "        \n",
    "        axes[row, 4].imshow(target, cmap='gray', vmin=0, vmax=1)\n",
    "        if row == 0:\n",
    "            axes[row, 4].set_title('Ground Truth')\n",
    "        axes[row, 4].axis('off')\n",
    "        \n",
    "        error = torch.abs(pred - target)\n",
    "        axes[row, 5].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n",
    "        if row == 0:\n",
    "            axes[row, 5].set_title('Error')\n",
    "        axes[row, 5].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(model, val_data, device, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_rollout(model, trajectories, device, traj_idx=0, context_len=5, rollout_steps=15):\n",
    "    \"\"\"Compare autoregressive rollout with ground truth.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    context = trajectories[traj_idx, :context_len].unsqueeze(0).to(device)\n",
    "    gt_future = trajectories[traj_idx, context_len:context_len+rollout_steps].cpu().numpy()\n",
    "    predicted = model.rollout(context, rollout_steps).cpu().squeeze(0).numpy()\n",
    "    \n",
    "    num_show = min(8, rollout_steps)\n",
    "    step_indices = np.linspace(0, rollout_steps-1, num_show, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_show, figsize=(2*num_show, 6))\n",
    "    \n",
    "    for col, t in enumerate(step_indices):\n",
    "        axes[0, col].imshow(np.clip(predicted[t], 0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "        if col == 0:\n",
    "            axes[0, col].set_ylabel('Predicted')\n",
    "        axes[0, col].set_title(f't+{t+1}')\n",
    "        axes[0, col].axis('off')\n",
    "        \n",
    "        axes[1, col].imshow(gt_future[t], cmap='gray', vmin=0, vmax=1)\n",
    "        if col == 0:\n",
    "            axes[1, col].set_ylabel('Ground Truth')\n",
    "        axes[1, col].axis('off')\n",
    "        \n",
    "        error = np.abs(np.clip(predicted[t], 0, 1) - gt_future[t])\n",
    "        axes[2, col].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n",
    "        if col == 0:\n",
    "            axes[2, col].set_ylabel('Error')\n",
    "        axes[2, col].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Autoregressive Rollout ({rollout_steps} steps, context={context_len})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # MSE over time\n",
    "    mse_over_time = np.mean((np.clip(predicted, 0, 1) - gt_future)**2, axis=(1, 2))\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, rollout_steps+1), mse_over_time)\n",
    "    plt.xlabel('Rollout Step')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('Prediction Error vs Rollout Length')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Test rollout with different context lengths\n",
    "for ctx_len in [3, 8, 15]:\n",
    "    print(f\"\\nContext length: {ctx_len}\")\n",
    "    visualize_rollout(model, val_data, device, traj_idx=0, context_len=ctx_len, rollout_steps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animated Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def animate_rollout_comparison(model, trajectories, device, traj_idx=0, context_len=5, rollout_steps=20):\n",
    "    \"\"\"Create side-by-side animation of prediction vs ground truth.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    context = trajectories[traj_idx, :context_len].unsqueeze(0).to(device)\n",
    "    gt_future = trajectories[traj_idx, context_len:context_len+rollout_steps].cpu().numpy()\n",
    "    predicted = model.rollout(context, rollout_steps).cpu().squeeze(0).numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 3.5))\n",
    "    \n",
    "    im_pred = axes[0].imshow(np.clip(predicted[0], 0, 1), cmap='gray', vmin=0, vmax=1)\n",
    "    axes[0].set_title('Predicted')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    im_gt = axes[1].imshow(gt_future[0], cmap='gray', vmin=0, vmax=1)\n",
    "    axes[1].set_title('Ground Truth')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    error = np.abs(np.clip(predicted[0], 0, 1) - gt_future[0])\n",
    "    im_err = axes[2].imshow(error, cmap='hot', vmin=0, vmax=0.5)\n",
    "    axes[2].set_title('Error')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    title = fig.suptitle('t+1')\n",
    "    \n",
    "    def update(frame):\n",
    "        im_pred.set_array(np.clip(predicted[frame], 0, 1))\n",
    "        im_gt.set_array(gt_future[frame])\n",
    "        error = np.abs(np.clip(predicted[frame], 0, 1) - gt_future[frame])\n",
    "        im_err.set_array(error)\n",
    "        title.set_text(f't+{frame+1}')\n",
    "        return [im_pred, im_gt, im_err, title]\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, update, frames=rollout_steps, interval=150, blit=False)\n",
    "    plt.close()\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "animate_rollout_comparison(model, val_data, device, traj_idx=0, context_len=5, rollout_steps=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis: Does the Model Learn Physics?\n",
    "\n",
    "### Latent Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def analyze_internal_representations(model, trajectories, device, num_traj=5):\n",
    "    \"\"\"Analyze what the transformer learns by looking at attention patterns.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # We can't easily extract latents since raw pixels go directly to transformer\n",
    "    # Instead, analyze the transformer's internal activations on sample trajectories\n",
    "    \n",
    "    fig, axes = plt.subplots(num_traj, 4, figsize=(12, 3*num_traj))\n",
    "    \n",
    "    for i in range(num_traj):\n",
    "        # Get a trajectory and run predictions with different context lengths\n",
    "        traj = trajectories[i]  # (T, H, W)\n",
    "        \n",
    "        mses = []\n",
    "        context_lengths = [3, 5, 10, 15, 20]\n",
    "        \n",
    "        for ctx_len in context_lengths:\n",
    "            if ctx_len >= len(traj) - 1:\n",
    "                continue\n",
    "            context = traj[:ctx_len].unsqueeze(0).to(device)\n",
    "            target = traj[ctx_len]\n",
    "            pred = model(context).cpu().squeeze(0)\n",
    "            mse = ((pred - target) ** 2).mean().item()\n",
    "            mses.append((ctx_len, mse))\n",
    "        \n",
    "        # Plot: trajectory snapshots\n",
    "        for j, t in enumerate([0, 5, 10, 15]):\n",
    "            if t < len(traj):\n",
    "                axes[i, j].imshow(traj[t].cpu(), cmap='gray', vmin=0, vmax=1)\n",
    "                axes[i, j].set_title(f't={t}')\n",
    "            axes[i, j].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Trajectories')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot MSE vs context length (aggregate)\n",
    "    all_mses = {ctx: [] for ctx in [3, 5, 10, 15, 20]}\n",
    "    \n",
    "    for i in range(min(50, len(trajectories))):\n",
    "        traj = trajectories[i]\n",
    "        for ctx_len in all_mses.keys():\n",
    "            if ctx_len >= len(traj) - 1:\n",
    "                continue\n",
    "            context = traj[:ctx_len].unsqueeze(0).to(device)\n",
    "            target = traj[ctx_len]\n",
    "            pred = model(context).cpu().squeeze(0)\n",
    "            mse = ((pred - target) ** 2).mean().item()\n",
    "            all_mses[ctx_len].append(mse)\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    ctx_lens = sorted(all_mses.keys())\n",
    "    means = [np.mean(all_mses[c]) if all_mses[c] else 0 for c in ctx_lens]\n",
    "    stds = [np.std(all_mses[c]) if all_mses[c] else 0 for c in ctx_lens]\n",
    "    \n",
    "    plt.errorbar(ctx_lens, means, yerr=stds, marker='o', capsize=5)\n",
    "    plt.xlabel('Context Length')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('Prediction Error vs Context Length')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "analyze_internal_representations(model, val_data, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'history': history,\n",
    "    'config': {\n",
    "        'frame_size': RESOLUTION,\n",
    "        'n_heads': N_HEADS,\n",
    "        'n_layers': N_LAYERS,\n",
    "        'dropout': DROPOUT,\n",
    "        'min_context': MIN_CONTEXT,\n",
    "        'max_context': MAX_CONTEXT,\n",
    "    }\n",
    "}\n",
    "torch.save(checkpoint, 'results/model_checkpoint.pt')\n",
    "print(\"Model saved to results/model_checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Checkpoints (VS Code users)\n",
    "\n",
    "Run this cell to download checkpoints to your local machine via the VS Code file browser, or zip them for easy download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip all checkpoints for easy download\n",
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "zip_name = f'checkpoints_{timestamp}'\n",
    "\n",
    "# Create zip of results folder\n",
    "shutil.make_archive(zip_name, 'zip', 'results')\n",
    "print(f\"Created {zip_name}.zip\")\n",
    "print(f\"Size: {os.path.getsize(f'{zip_name}.zip') / 1024 / 1024:.1f} MB\")\n",
    "print(\"\\nDownload via VS Code file browser (left panel) or run:\")\n",
    "print(f\"  !cp {zip_name}.zip /content/\")\n",
    "\n",
    "# List what's in results\n",
    "print(\"\\nCheckpoints in results/:\")\n",
    "!ls -lh results/*.pt 2>/dev/null || echo \"No .pt files yet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key metrics to track:\n",
    "- **Single-step MSE**: How well does model predict 1 frame ahead?\n",
    "- **Rollout degradation**: How fast does error grow with longer rollouts?\n",
    "- **Visual quality**: Do predictions look like valid physics?\n",
    "- **Latent structure**: Does latent space encode position/velocity?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}