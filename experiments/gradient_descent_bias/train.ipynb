{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Bias in Overcomplete Autoencoders\n",
    "\n",
    "**Question**: Does gradient descent have an implicit bias toward semantically meaningful representations, even when there's no information-theoretic pressure to compress?\n",
    "\n",
    "We train autoencoders where the hidden dimension is >= input dimension (784 for MNIST). With no bottleneck, the model could learn any arbitrary mapping that reconstructs perfectly. What does gradient descent actually find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Colab\n",
    "import os\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not os.path.exists('/content/MNIST_AI'):\n",
    "        !git clone https://github.com/Caleb-Briggs/MNIST_AI.git\n",
    "    %cd /content/MNIST_AI\n",
    "    import sys\n",
    "    sys.path.append('/content/MNIST_AI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nfrom IPython.display import HTML\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom typing import Optional\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom shared.utils.data import load_mnist, get_device\n\ndevice = get_device()\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OvercompleteAutoencoder(nn.Module):\n",
    "    \"\"\"Simple autoencoder with configurable hidden dimension.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 784, hidden_dim: int = 1024):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.activation(self.encoder(x))\n",
    "    \n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_encoder_weights(model: OvercompleteAutoencoder, \n",
    "                               num_units: int = 64,\n",
    "                               figsize: tuple = (12, 12),\n",
    "                               title: str = \"Encoder Weights\"):\n",
    "    \"\"\"\n",
    "    Visualize encoder weight vectors reshaped as 28x28 images.\n",
    "    Each hidden unit has a 784-dim weight vector that can be viewed as an image.\n",
    "    \n",
    "    If weights look like single bright pixels -> identity-like\n",
    "    If weights look like edges/strokes -> learning features\n",
    "    If weights look like noise -> random solution\n",
    "    \"\"\"\n",
    "    weights = model.encoder.weight.detach().cpu().numpy()  # (hidden_dim, 784)\n",
    "    \n",
    "    # Select subset of units to display\n",
    "    n_display = min(num_units, weights.shape[0])\n",
    "    grid_size = int(np.ceil(np.sqrt(n_display)))\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(grid_size * grid_size):\n",
    "        if i < n_display:\n",
    "            w = weights[i].reshape(28, 28)\n",
    "            # Normalize for visualization\n",
    "            vmax = max(abs(w.min()), abs(w.max()))\n",
    "            axes[i].imshow(w, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(model: OvercompleteAutoencoder,\n",
    "                               images: torch.Tensor,\n",
    "                               num_examples: int = 10,\n",
    "                               figsize: tuple = (15, 3)):\n",
    "    \"\"\"Show original vs reconstructed images.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        flat = images[:num_examples].view(num_examples, -1)\n",
    "        recon, _ = model(flat)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_examples, figsize=figsize)\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        axes[0, i].imshow(images[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(recon[i].cpu().view(28, 28), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Reconstructed', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_probe_accuracy(representations: np.ndarray,\n",
    "                                   labels: np.ndarray,\n",
    "                                   n_components: Optional[int] = 50,\n",
    "                                   train_size: int = 5000,\n",
    "                                   test_size: int = 1000) -> float:\n",
    "    \"\"\"\n",
    "    Train a linear classifier on representations and return test accuracy.\n",
    "    \n",
    "    Uses PCA to control for dimensionality differences between raw pixels\n",
    "    and latent representations of different sizes.\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    X_train, y_train = representations[:train_size], labels[:train_size]\n",
    "    X_test, y_test = representations[train_size:train_size+test_size], labels[train_size:train_size+test_size]\n",
    "    \n",
    "    # Apply PCA if requested (to control for dimensionality)\n",
    "    if n_components is not None and n_components < representations.shape[1]:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_train = pca.fit_transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "    \n",
    "    # Train linear classifier\n",
    "    clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    return clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight_statistics(model: OvercompleteAutoencoder) -> dict:\n",
    "    \"\"\"Compute statistics about the weight matrices.\"\"\"\n",
    "    enc_weights = model.encoder.weight.detach().cpu().numpy()\n",
    "    dec_weights = model.decoder.weight.detach().cpu().numpy()\n",
    "    \n",
    "    # Singular values (for rank analysis)\n",
    "    enc_svd = np.linalg.svd(enc_weights, compute_uv=False)\n",
    "    dec_svd = np.linalg.svd(dec_weights, compute_uv=False)\n",
    "    \n",
    "    # Effective rank (entropy-based)\n",
    "    def effective_rank(s):\n",
    "        s = s / s.sum()\n",
    "        s = s[s > 1e-10]  # Avoid log(0)\n",
    "        return np.exp(-np.sum(s * np.log(s)))\n",
    "    \n",
    "    return {\n",
    "        'encoder_singular_values': enc_svd,\n",
    "        'decoder_singular_values': dec_svd,\n",
    "        'encoder_effective_rank': effective_rank(enc_svd),\n",
    "        'decoder_effective_rank': effective_rank(dec_svd),\n",
    "        'encoder_frobenius_norm': np.linalg.norm(enc_weights, 'fro'),\n",
    "        'decoder_frobenius_norm': np.linalg.norm(dec_weights, 'fro'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class TrainingHistory:\n    \"\"\"Stores weight snapshots and metrics in memory.\"\"\"\n    \n    def __init__(self):\n        self.losses = []\n        self.epochs = []\n        self.encoder_weights = []  # List of (epoch, weights) - numpy arrays\n        self.decoder_weights = []  # List of (epoch, weights) - numpy arrays\n        self.encoder_bias = []     # List of (epoch, bias)\n        self.decoder_bias = []     # List of (epoch, bias)\n        self.linear_probe_latent = []\n        self.linear_probe_raw = None\n        self._data_sample = None   # Cache a sample of data for computing activations\n    \n    def save_weights(self, model: OvercompleteAutoencoder, epoch: int):\n        \"\"\"Save encoder and decoder weights and biases as numpy arrays.\"\"\"\n        enc_w = model.encoder.weight.detach().cpu().numpy().copy()\n        dec_w = model.decoder.weight.detach().cpu().numpy().copy()\n        enc_b = model.encoder.bias.detach().cpu().numpy().copy()\n        dec_b = model.decoder.bias.detach().cpu().numpy().copy()\n        self.encoder_weights.append((epoch, enc_w))\n        self.decoder_weights.append((epoch, dec_w))\n        self.encoder_bias.append((epoch, enc_b))\n        self.decoder_bias.append((epoch, dec_b))\n    \n    def set_data_sample(self, data: np.ndarray):\n        \"\"\"Store a sample of data for computing activations.\"\"\"\n        self._data_sample = data\n    \n    def get_importance_order(self, epoch: int) -> np.ndarray:\n        \"\"\"\n        Get unit indices sorted by importance (most important first).\n        \n        Importance = ||encoder_weights|| × mean(activation) × ||decoder_weights||\n        \n        This captures:\n        - ||encoder_weights||: how much input the unit looks at (L2 norm)\n        - mean(activation): how strongly the unit fires on average\n        - ||decoder_weights||: how much the unit contributes to output\n        \"\"\"\n        # Find weights/biases for this epoch\n        enc_w, dec_w, enc_b = None, None, None\n        for e, w in self.encoder_weights:\n            if e == epoch:\n                enc_w = w\n                break\n        for e, w in self.decoder_weights:\n            if e == epoch:\n                dec_w = w\n                break\n        for e, b in self.encoder_bias:\n            if e == epoch:\n                enc_b = b\n                break\n        \n        if enc_w is None or dec_w is None or enc_b is None:\n            return None\n        \n        # Encoder weight norm per unit\n        # enc_w shape: (hidden_dim, 784)\n        enc_norm = np.linalg.norm(enc_w, axis=1)  # (hidden_dim,)\n        \n        # Decoder weight norm per unit\n        # dec_w shape: (784, hidden_dim)\n        dec_norm = np.linalg.norm(dec_w, axis=0)  # (hidden_dim,)\n        \n        # Mean activation per unit\n        if self._data_sample is not None:\n            activations = self._data_sample @ enc_w.T + enc_b  # (N, hidden_dim)\n            activations = np.maximum(activations, 0)  # ReLU\n            mean_act = np.mean(activations, axis=0)  # (hidden_dim,)\n        else:\n            mean_act = np.ones(enc_w.shape[0])\n        \n        # Importance = encoder_norm × mean_activation × decoder_norm\n        importance = enc_norm * mean_act * dec_norm\n        \n        return np.argsort(importance)[::-1]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_autoencoder(\n    hidden_dim: int = 1024,\n    epochs: int = 50,\n    lr: float = 1e-3,\n    batch_size: int = 256,\n    snapshot_every: int = 1,\n    compute_probes: bool = True,\n    probe_every: int = 5,\n    probe_n_components: int = 50,\n    seed: int = 42,\n    verbose: bool = True,\n    single_pass: bool = False,\n    snapshots_per_pass: int = 50,\n    val_split: float = 0.1,\n) -> tuple[OvercompleteAutoencoder, TrainingHistory]:\n    \"\"\"\n    Train an overcomplete autoencoder and track metrics in memory.\n\n    Args:\n        single_pass: If True, each image is seen exactly once (no overfitting).\n                    The 'epochs' parameter is ignored; instead we take\n                    'snapshots_per_pass' evenly spaced snapshots through the data.\n        snapshots_per_pass: Number of weight snapshots to save during single-pass training.\n        val_split: Fraction of data to hold out for validation (default 10%).\n    \"\"\"\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    # Load data\n    images, labels = load_mnist(device, train=True)\n    images_flat = images.view(images.size(0), -1)  # (60000, 784)\n    labels_np = labels.cpu().numpy()\n\n    # Split into train and validation\n    n_total = len(images_flat)\n    n_val = int(n_total * val_split)\n    n_train = n_total - n_val\n\n    # Shuffle indices for split\n    perm = torch.randperm(n_total, device=device)\n    train_indices = perm[:n_train]\n    val_indices = perm[n_train:]\n\n    train_images = images_flat[train_indices]\n    val_images = images_flat[val_indices]\n\n    if verbose:\n        print(f\"Training set: {n_train} images\")\n        print(f\"Validation set: {n_val} images\")\n\n    # Initialize model\n    model = OvercompleteAutoencoder(input_dim=784, hidden_dim=hidden_dim).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    history = TrainingHistory()\n    history.val_losses = []  # Track validation loss\n\n    # Store data sample for importance computation (use subset to save memory)\n    history.set_data_sample(train_images[:5000].cpu().numpy())\n\n    # Compute raw pixel baseline once\n    if compute_probes:\n        raw_pixels = images_flat.cpu().numpy()\n        history.linear_probe_raw = compute_linear_probe_accuracy(\n            raw_pixels, labels_np, n_components=probe_n_components\n        )\n        if verbose:\n            print(f\"Raw pixel linear probe accuracy: {history.linear_probe_raw:.4f}\")\n\n    # Helper to compute validation loss\n    def compute_val_loss():\n        model.eval()\n        with torch.no_grad():\n            recon, _ = model(val_images)\n            val_loss = criterion(recon, val_images).item()\n        model.train()\n        return val_loss\n\n    # Save initial state (epoch 0)\n    history.save_weights(model, 0)\n    history.epochs.append(0)\n    history.losses.append(float('inf'))\n    history.val_losses.append(compute_val_loss())\n\n    if compute_probes:\n        model.eval()\n        with torch.no_grad():\n            _, latent = model(images_flat)\n        acc = compute_linear_probe_accuracy(\n            latent.cpu().numpy(), labels_np, n_components=probe_n_components\n        )\n        history.linear_probe_latent.append((0, acc))\n        if verbose:\n            print(f\"Epoch 0 - Val loss: {history.val_losses[-1]:.6f} - Latent probe: {acc:.4f}\")\n\n    n_batches = len(train_images) // batch_size\n\n    if single_pass:\n        # Single-pass training: each image seen exactly once\n        if verbose:\n            print(f\"\\nSingle-pass mode: {n_train} train images, {n_batches} batches\")\n            print(f\"Taking {snapshots_per_pass} snapshots during training\\n\")\n\n        # Shuffle training data once at the start\n        train_perm = torch.randperm(n_train, device=device)\n\n        # Calculate snapshot intervals (in terms of batches)\n        snapshot_batches = set(int(i * n_batches / snapshots_per_pass) for i in range(1, snapshots_per_pass + 1))\n\n        model.train()\n        running_loss = 0.0\n        loss_count = 0\n\n        for batch_idx in range(n_batches):\n            idx = train_perm[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n            batch = train_images[idx]\n\n            optimizer.zero_grad()\n            recon, _ = model(batch)\n            loss = criterion(recon, batch)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            loss_count += 1\n\n            # Check if we should snapshot\n            if batch_idx + 1 in snapshot_batches:\n                # Use images seen as the \"epoch\" marker\n                images_seen = (batch_idx + 1) * batch_size\n                avg_loss = running_loss / loss_count\n                val_loss = compute_val_loss()\n\n                history.losses.append(avg_loss)\n                history.val_losses.append(val_loss)\n                history.epochs.append(images_seen)\n                history.save_weights(model, images_seen)\n\n                if verbose:\n                    print(f\"Images: {images_seen:,} / {n_train:,} - Train: {avg_loss:.6f} - Val: {val_loss:.6f}\")\n\n                running_loss = 0.0\n                loss_count = 0\n\n        # Final probe\n        if compute_probes:\n            model.eval()\n            with torch.no_grad():\n                _, latent = model(images_flat)\n            acc = compute_linear_probe_accuracy(\n                latent.cpu().numpy(), labels_np, n_components=probe_n_components\n            )\n            history.linear_probe_latent.append((n_train, acc))\n            if verbose:\n                print(f\"\\nFinal latent probe accuracy: {acc:.4f}\")\n\n    else:\n        # Standard multi-epoch training\n        for epoch in range(1, epochs + 1):\n            model.train()\n            epoch_loss = 0.0\n\n            # Shuffle each epoch\n            train_perm = torch.randperm(n_train, device=device)\n\n            for i in range(n_batches):\n                idx = train_perm[i * batch_size:(i + 1) * batch_size]\n                batch = train_images[idx]\n\n                optimizer.zero_grad()\n                recon, _ = model(batch)\n                loss = criterion(recon, batch)\n                loss.backward()\n                optimizer.step()\n\n                epoch_loss += loss.item()\n\n            epoch_loss /= n_batches\n            val_loss = compute_val_loss()\n\n            history.losses.append(epoch_loss)\n            history.val_losses.append(val_loss)\n            history.epochs.append(epoch)\n\n            # Save weight snapshot\n            if epoch % snapshot_every == 0:\n                history.save_weights(model, epoch)\n\n            # Compute linear probe (less frequently - it's expensive)\n            if compute_probes and epoch % probe_every == 0:\n                model.eval()\n                with torch.no_grad():\n                    _, latent = model(images_flat)\n                acc = compute_linear_probe_accuracy(\n                    latent.cpu().numpy(), labels_np, n_components=probe_n_components\n                )\n                history.linear_probe_latent.append((epoch, acc))\n                if verbose:\n                    print(f\"Epoch {epoch}/{epochs} - Train: {epoch_loss:.6f} - Val: {val_loss:.6f} - Probe: {acc:.4f}\")\n            elif verbose:\n                print(f\"Epoch {epoch}/{epochs} - Train: {epoch_loss:.6f} - Val: {val_loss:.6f}\")\n\n    return model, history"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_training_summary(history: TrainingHistory, figsize: tuple = (12, 4)):\n    \"\"\"Plot loss curves (train + validation) and linear probe comparison.\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=figsize)\n\n    # Loss curves\n    axes[0].plot(history.epochs, history.losses, label='Train')\n    if hasattr(history, 'val_losses') and history.val_losses:\n        axes[0].plot(history.epochs, history.val_losses, label='Validation')\n    axes[0].set_xlabel('Images seen' if history.epochs[-1] > 100 else 'Epoch')\n    axes[0].set_ylabel('MSE Loss')\n    axes[0].set_title('Reconstruction Loss')\n    axes[0].set_yscale('log')\n    axes[0].legend()\n\n    # Linear probe comparison\n    if history.linear_probe_latent:\n        probe_epochs = [e for e, _ in history.linear_probe_latent]\n        probe_accs = [a for _, a in history.linear_probe_latent]\n        axes[1].plot(probe_epochs, probe_accs, 'b-o', label='Latent')\n        axes[1].axhline(y=history.linear_probe_raw, color='r', linestyle='--', label='Raw pixels')\n        axes[1].set_xlabel('Images seen' if history.epochs[-1] > 100 else 'Epoch')\n        axes[1].set_ylabel('Accuracy')\n        axes[1].set_title('Linear Probe Accuracy')\n        axes[1].legend()\n\n    plt.tight_layout()\n    return fig"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_weight_evolution(history: TrainingHistory,\n                                num_units: int = 8,\n                                sort_by_importance: bool = True,\n                                figsize_per_unit: float = 1.5):\n    \"\"\"\n    Visualize how hidden units evolve across ALL epochs.\n    \n    Creates a grid: rows = hidden units, columns = epochs.\n    \n    If sort_by_importance=True, units are ordered by their importance \n    (decoder weight norm) at EACH epoch. This means the same row might \n    show different units across epochs - you're watching \"the most important\n    unit\", \"second most important\", etc.\n    \n    If sort_by_importance=False, shows fixed unit indices 0, 1, 2, ...\n    \"\"\"\n    n_snapshots = len(history.encoder_weights)\n    \n    fig, axes = plt.subplots(\n        num_units, n_snapshots,\n        figsize=(figsize_per_unit * n_snapshots, figsize_per_unit * num_units)\n    )\n    \n    if num_units == 1:\n        axes = axes.reshape(1, -1)\n    \n    for col, (epoch, enc_weights) in enumerate(history.encoder_weights):\n        # Get unit ordering for this epoch\n        if sort_by_importance:\n            order = history.get_importance_order(epoch)\n            if order is None:\n                order = np.arange(enc_weights.shape[0])\n        else:\n            order = np.arange(enc_weights.shape[0])\n        \n        for row in range(num_units):\n            unit_idx = order[row]\n            w = enc_weights[unit_idx].reshape(28, 28)\n            vmax = max(abs(w.min()), abs(w.max()))\n            axes[row, col].imshow(w, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n            axes[row, col].axis('off')\n            \n            if row == 0:\n                axes[row, col].set_title(f'E{epoch}', fontsize=8)\n            if col == 0:\n                label = f'Rank {row+1}' if sort_by_importance else f'Unit {row}'\n                axes[row, col].set_ylabel(label, fontsize=8)\n    \n    title = 'Weight Evolution (sorted by importance per epoch)' if sort_by_importance else 'Weight Evolution (fixed unit indices)'\n    plt.suptitle(title, fontsize=10)\n    plt.tight_layout()\n    return fig\n\n\ndef visualize_all_weights_at_epoch(history: TrainingHistory,\n                                    epoch: int,\n                                    num_units: int = 64,\n                                    sort_by_importance: bool = True,\n                                    figsize: tuple = (12, 12)):\n    \"\"\"\n    Show a grid of encoder weights at a specific epoch.\n    If sort_by_importance=True, most important units shown first (top-left to bottom-right).\n    \"\"\"\n    # Find the snapshot for this epoch\n    enc_weights = None\n    for e, w in history.encoder_weights:\n        if e == epoch:\n            enc_weights = w\n            break\n    \n    if enc_weights is None:\n        print(f\"No snapshot for epoch {epoch}\")\n        return None\n    \n    # Get ordering\n    if sort_by_importance:\n        order = history.get_importance_order(epoch)\n        if order is None:\n            order = np.arange(enc_weights.shape[0])\n    else:\n        order = np.arange(enc_weights.shape[0])\n    \n    n_display = min(num_units, enc_weights.shape[0])\n    grid_size = int(np.ceil(np.sqrt(n_display)))\n    \n    fig, axes = plt.subplots(grid_size, grid_size, figsize=figsize)\n    axes = axes.flatten()\n    \n    for i in range(grid_size * grid_size):\n        if i < n_display:\n            unit_idx = order[i]\n            w = enc_weights[unit_idx].reshape(28, 28)\n            vmax = max(abs(w.min()), abs(w.max()))\n            axes[i].imshow(w, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n        axes[i].axis('off')\n    \n    sort_label = \" (by importance)\" if sort_by_importance else \"\"\n    plt.suptitle(f'Encoder Weights at Epoch {epoch}{sort_label}', fontsize=14)\n    plt.tight_layout()\n    return fig\n\n\ndef visualize_epoch_grid(history: TrainingHistory,\n                         epochs_to_show: list[int] = None,\n                         num_units: int = 16,\n                         sort_by_importance: bool = True,\n                         figsize_scale: float = 3):\n    \"\"\"\n    Show weight grids for multiple epochs side by side.\n    Each column is an epoch. Units sorted by importance at each epoch.\n    \"\"\"\n    if epochs_to_show is None:\n        epochs_to_show = [e for e, _ in history.encoder_weights]\n    \n    n_epochs = len(epochs_to_show)\n    grid_size = int(np.ceil(np.sqrt(num_units)))\n    \n    fig, axes = plt.subplots(1, n_epochs, figsize=(figsize_scale * n_epochs, figsize_scale))\n    if n_epochs == 1:\n        axes = [axes]\n    \n    for ax, target_epoch in zip(axes, epochs_to_show):\n        # Find weights for this epoch\n        enc_weights = None\n        for e, w in history.encoder_weights:\n            if e == target_epoch:\n                enc_weights = w\n                break\n        \n        if enc_weights is None:\n            ax.set_title(f'Epoch {target_epoch}\\n(no data)')\n            ax.axis('off')\n            continue\n        \n        # Get ordering\n        if sort_by_importance:\n            order = history.get_importance_order(target_epoch)\n            if order is None:\n                order = np.arange(enc_weights.shape[0])\n        else:\n            order = np.arange(enc_weights.shape[0])\n        \n        # Create grid image\n        grid = np.zeros((grid_size * 28, grid_size * 28))\n        for i in range(min(num_units, enc_weights.shape[0])):\n            row, col = i // grid_size, i % grid_size\n            unit_idx = order[i]\n            w = enc_weights[unit_idx].reshape(28, 28)\n            w = (w - w.min()) / (w.max() - w.min() + 1e-8)\n            grid[row*28:(row+1)*28, col*28:(col+1)*28] = w\n        \n        ax.imshow(grid, cmap='gray')\n        ax.set_title(f'Epoch {target_epoch}')\n        ax.axis('off')\n    \n    plt.tight_layout()\n    return fig"
  },
  {
   "cell_type": "code",
   "source": "def create_weight_animation(history: TrainingHistory,\n                             num_units: int = 16,\n                             sort_by_importance: bool = True,\n                             interval: int = 200):\n    \"\"\"\n    Create an animation showing weights evolving over training.\n    Units sorted by importance at each frame.\n    Returns an HTML object that plays in the notebook.\n    \"\"\"\n    grid_size = int(np.ceil(np.sqrt(num_units)))\n    \n    fig, ax = plt.subplots(figsize=(6, 6))\n    \n    def get_grid(frame_idx):\n        epoch, enc_weights = history.encoder_weights[frame_idx]\n        \n        if sort_by_importance:\n            order = history.get_importance_order(epoch)\n            if order is None:\n                order = np.arange(enc_weights.shape[0])\n        else:\n            order = np.arange(enc_weights.shape[0])\n        \n        grid = np.zeros((grid_size * 28, grid_size * 28))\n        for i in range(min(num_units, enc_weights.shape[0])):\n            row, col = i // grid_size, i % grid_size\n            unit_idx = order[i]\n            w = enc_weights[unit_idx].reshape(28, 28)\n            w = (w - w.min()) / (w.max() - w.min() + 1e-8)\n            grid[row*28:(row+1)*28, col*28:(col+1)*28] = w\n        return grid, epoch\n    \n    # Initialize with first frame\n    grid, epoch = get_grid(0)\n    im = ax.imshow(grid, cmap='gray', animated=True)\n    title = ax.set_title(f'Epoch {epoch}')\n    ax.axis('off')\n    \n    def update(frame):\n        grid, epoch = get_grid(frame)\n        im.set_array(grid)\n        title.set_text(f'Epoch {epoch}')\n        return [im, title]\n    \n    anim = animation.FuncAnimation(\n        fig, update,\n        frames=len(history.encoder_weights),\n        interval=interval,\n        blit=True\n    )\n    plt.close(fig)\n    return HTML(anim.to_jshtml())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def plot_importance_distribution(history: TrainingHistory,\n                                  epochs_to_show: list[int] = None,\n                                  figsize: tuple = (12, 4)):\n    \"\"\"\n    Plot the distribution of unit importances at different epochs.\n    Importance = ||encoder_weights|| × mean(activation) × ||decoder_weights||\n    \"\"\"\n    if epochs_to_show is None:\n        all_epochs = [e for e, _ in history.decoder_weights]\n        epochs_to_show = [all_epochs[0], all_epochs[len(all_epochs)//2], all_epochs[-1]]\n    \n    fig, axes = plt.subplots(1, len(epochs_to_show), figsize=figsize)\n    if len(epochs_to_show) == 1:\n        axes = [axes]\n    \n    for ax, target_epoch in zip(axes, epochs_to_show):\n        # Get weights and bias\n        enc_w, dec_w, enc_b = None, None, None\n        for e, w in history.encoder_weights:\n            if e == target_epoch:\n                enc_w = w\n                break\n        for e, w in history.decoder_weights:\n            if e == target_epoch:\n                dec_w = w\n                break\n        for e, b in history.encoder_bias:\n            if e == target_epoch:\n                enc_b = b\n                break\n        \n        if enc_w is None or dec_w is None or enc_b is None:\n            continue\n        \n        # Compute importance = enc_norm × mean_activation × dec_norm\n        enc_norm = np.linalg.norm(enc_w, axis=1)\n        dec_norm = np.linalg.norm(dec_w, axis=0)\n        \n        if history._data_sample is not None:\n            activations = history._data_sample @ enc_w.T + enc_b\n            activations = np.maximum(activations, 0)\n            mean_act = np.mean(activations, axis=0)\n        else:\n            mean_act = np.ones(enc_w.shape[0])\n        \n        importance = enc_norm * mean_act * dec_norm\n        importance_sorted = np.sort(importance)[::-1]\n        \n        ax.bar(range(len(importance_sorted)), importance_sorted, width=1.0)\n        ax.set_xlabel('Unit rank')\n        ax.set_ylabel('Importance')\n        ax.set_title(f'Epoch {target_epoch}')\n    \n    plt.suptitle('Importance = ||enc|| × activation × ||dec||', fontsize=12)\n    plt.tight_layout()\n    return fig\n\n\ndef track_top_units(history: TrainingHistory, top_k: int = 10):\n    \"\"\"\n    Track which unit indices are in the top-k most important over training.\n    \"\"\"\n    tracking = []\n    for epoch, _ in history.decoder_weights:\n        order = history.get_importance_order(epoch)\n        if order is not None:\n            tracking.append((epoch, order[:top_k].tolist()))\n    return tracking",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Export data as JSON for the HTML explorer\nimport json\n\ndef export_explorer_data(history: TrainingHistory,\n                         epochs: list[int],\n                         sample_images: np.ndarray,\n                         sample_labels: np.ndarray = None):\n    \"\"\"\n    Export all data needed for the HTML explorer as JSON.\n    Supports multiple epochs for exploring training dynamics.\n    \"\"\"\n    epochs_data = []\n    \n    for epoch in epochs:\n        # Get weights for this epoch\n        enc_w, dec_w, enc_b, dec_b = None, None, None, None\n        for e, w in history.encoder_weights:\n            if e == epoch:\n                enc_w = w\n                break\n        for e, w in history.decoder_weights:\n            if e == epoch:\n                dec_w = w\n                break\n        for e, b in history.encoder_bias:\n            if e == epoch:\n                enc_b = b\n                break\n        for e, b in history.decoder_bias:\n            if e == epoch:\n                dec_b = b\n                break\n        \n        if enc_w is None:\n            print(f\"No data for epoch {epoch}, skipping\")\n            continue\n        \n        # Get importance ordering and scores\n        order = history.get_importance_order(epoch)\n        \n        # Compute importance scores for ALL units\n        enc_norm = np.linalg.norm(enc_w, axis=1)\n        dec_norm = np.linalg.norm(dec_w, axis=0)\n        if history._data_sample is not None:\n            activations = history._data_sample @ enc_w.T + enc_b\n            activations = np.maximum(activations, 0)\n            mean_act = np.mean(activations, axis=0)\n        else:\n            mean_act = np.ones(enc_w.shape[0])\n        importance = enc_norm * mean_act * dec_norm\n        \n        epoch_data = {\n            'epoch': epoch,\n            'importance_order': order.tolist(),\n            'importance_scores': importance.tolist(),\n            'enc_weights': enc_w.tolist(),\n            'dec_weights': dec_w.T.tolist(),\n            'enc_bias': enc_b.tolist(),\n            'dec_bias': dec_b.tolist(),\n            'enc_norms': enc_norm.tolist(),\n            'dec_norms': dec_norm.tolist(),\n            'mean_activations': mean_act.tolist(),\n        }\n        epochs_data.append(epoch_data)\n        print(f\"  Epoch {epoch}: done\")\n    \n    data = {\n        'epochs': epochs_data,\n        'available_epochs': [ed['epoch'] for ed in epochs_data],\n        'hidden_dim': int(history.encoder_weights[0][1].shape[0]),\n        'input_dim': 784,\n        'sample_images': sample_images.tolist(),\n        'sample_labels': sample_labels.tolist() if sample_labels is not None else None,\n    }\n    \n    json_str = json.dumps(data)\n    print(f\"\\nJSON size: {len(json_str) / 1024 / 1024:.2f} MB\")\n    print(f\"Epochs saved: {data['available_epochs']}\")\n    print(f\"Hidden units: {data['hidden_dim']}\")\n    print(f\"Sample images: {len(sample_images)}\")\n    \n    return json_str",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nCONFIG = {\n    'hidden_dim': 1024,      # Try: 784, 1024, 2048\n    'lr': 1e-3,\n    'batch_size': 256,\n    'seed': 17,\n    'val_split': 0.1,        # 10% held out for validation\n    \n    # Single-pass mode: each image seen exactly once (no overfitting)\n    'single_pass': True,\n    'snapshots_per_pass': 50,  # Number of checkpoints during the single pass\n    \n    # These are only used if single_pass=False:\n    'epochs': 50,\n    'snapshot_every': 1,\n    'probe_every': 5,\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train_autoencoder(**CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "plot_training_summary(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Watch individual units evolve across ALL epochs\n# Each row is one hidden unit, each column is an epoch\nvisualize_weight_evolution(history, num_units=8)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Side-by-side comparison at selected checkpoints\n# Automatically picks evenly spaced checkpoints from what's available\navailable = [e for e, _ in history.encoder_weights]\nn = len(available)\nindices = [0, n//5, 2*n//5, 3*n//5, 4*n//5, n-1]\nepochs_to_show = [available[i] for i in indices]\nvisualize_epoch_grid(history, epochs_to_show=epochs_to_show, num_units=16)"
  },
  {
   "cell_type": "code",
   "source": "# Detailed view: all 64 units at final epoch\nfinal_epoch = history.encoder_weights[-1][0]\nvisualize_all_weights_at_epoch(history, epoch=final_epoch, num_units=64)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# How importance is distributed across units at different checkpoints\navailable = [e for e, _ in history.encoder_weights]\nn = len(available)\nepochs_to_show = [available[0], available[n//3], available[2*n//3], available[-1]]\nplot_importance_distribution(history, epochs_to_show=epochs_to_show)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Animated view - watch weights evolve over training\ncreate_weight_animation(history, num_units=16, interval=150)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Inspect an early checkpoint - change the index to explore different points\navailable = [e for e, _ in history.encoder_weights]\nearly_checkpoint = available[min(5, len(available)-1)]  # 5th checkpoint or last if fewer\nprint(f\"Showing checkpoint: {early_checkpoint}\")\nvisualize_all_weights_at_epoch(history, epoch=early_checkpoint, num_units=64)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction quality\n",
    "images, labels = load_mnist(device, train=False)\n",
    "visualize_reconstructions(model, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Comparison Across Hidden Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_size_comparison(hidden_dims: list[int] = [784, 1024, 2048],\n",
    "                        epochs: int = 30,\n",
    "                        **kwargs):\n",
    "    \"\"\"Train autoencoders with different hidden sizes and compare.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for dim in hidden_dims:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training with hidden_dim={dim}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        model, history = train_autoencoder(\n",
    "            hidden_dim=dim,\n",
    "            epochs=epochs,\n",
    "            **kwargs\n",
    "        )\n",
    "        results[dim] = {'model': model, 'history': history}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run comparison (takes longer)\n",
    "# results = run_size_comparison(\n",
    "#     hidden_dims=[784, 1024, 2048],\n",
    "#     epochs=30,\n",
    "#     checkpoint_every=10,\n",
    "#     seed=17\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results if run\n",
    "# for dim, res in results.items():\n",
    "#     print(f\"\\nhidden_dim={dim}:\")\n",
    "#     print(f\"  Final loss: {res['history'].losses[-1]:.6f}\")\n",
    "#     print(f\"  Raw probe: {res['history'].linear_probe_raw:.4f}\")\n",
    "#     print(f\"  Final latent probe: {res['history'].linear_probe_latent[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Export for HTML Explorer\n\nExport trained weights as JSON for use with `explorer.html`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Export JSON data for the HTML explorer\n# In single_pass mode, \"epochs\" are actually images_seen counts\n\n# Get test images for the explorer\ntest_images, test_labels = load_mnist(device, train=False)\ntest_flat = test_images[:100].view(100, -1).cpu().numpy()\ntest_labels_np = test_labels[:100].cpu().numpy()\n\n# Get all available epochs/checkpoints from history\navailable_epochs = [e for e, _ in history.encoder_weights]\nprint(f\"Available checkpoints: {len(available_epochs)}\")\nprint(f\"Range: {available_epochs[0]} to {available_epochs[-1]}\")\n\n# Select a subset for export (to keep JSON size manageable)\n# Take ~15 checkpoints: dense early, sparse later\nif len(available_epochs) > 15:\n    # Include first few, then logarithmically space the rest\n    n = len(available_epochs)\n    indices = [0, 1, 2, 3, 4]  # First 5\n    indices += [int(n * i / 10) for i in range(1, 10)]  # 10%, 20%, ..., 90%\n    indices.append(n - 1)  # Last one\n    indices = sorted(set(i for i in indices if i < n))\n    epochs_to_export = [available_epochs[i] for i in indices]\nelse:\n    epochs_to_export = available_epochs\n\nprint(f\"Exporting {len(epochs_to_export)} checkpoints: {epochs_to_export}\")\n\njson_str = export_explorer_data(\n    history,\n    epochs=epochs_to_export,\n    sample_images=test_flat,\n    sample_labels=test_labels_np\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save and download JSON file\nfilename = \"explorer_data.json\"\n\nwith open(filename, 'w') as f:\n    f.write(json_str)\n\nprint(f\"Saved {filename} ({len(json_str) / 1024 / 1024:.2f} MB)\")\n\n# Download via Colab\nfrom google.colab import files\nfiles.download(filename)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}