{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Clustering Evolution\n",
    "\n",
    "Track how network representations evolve during training:\n",
    "1. Cluster raw pixels by cosine similarity\n",
    "2. Extract activations at each layer during training\n",
    "3. Visualize clustering quality over time\n",
    "4. See how network learns to separate digits in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MNIST_AI'...\n",
      "remote: Enumerating objects: 54, done.\u001b[K\n",
      "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
      "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
      "remote: Total 54 (delta 11), reused 51 (delta 8), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (54/54), 18.82 MiB | 14.02 MiB/s, done.\n",
      "Resolving deltas: 100% (11/11), done.\n",
      "/content/MNIST_AI\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Caleb-Briggs/MNIST_AI.git\n",
    "%cd MNIST_AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, adjusted_rand_score\nfrom tqdm.auto import tqdm\nimport seaborn as sns\n\nimport sys\nsys.path.append('/content/MNIST_AI')\n\nfrom shared.utils.data import load_mnist, get_device\nfrom shared.utils.models import SmallCNN\n\ndevice = get_device()\nprint(f\"Device: {device}\")\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Config\nTRAIN_SIZE = 5000  # Train on more data than correlation experiment\nEVAL_SIZE = 1000   # Subset for visualization\nMAX_EPOCHS = 100\nLR = 1e-3\nCHECKPOINT_EPOCHS = list(range(0, 101, 2))  # Every 2 epochs for smooth animation\nSEED = 17  # Different seed each experiment (not 42!)\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "images, labels = load_mnist(device, train=True)\n",
    "test_images, test_labels = load_mnist(device, train=False)\n",
    "\n",
    "# Sample training set (balanced across digits)\n",
    "train_indices = []\n",
    "samples_per_digit = TRAIN_SIZE // 10\n",
    "for d in range(10):\n",
    "    digit_mask = labels == d\n",
    "    digit_indices = torch.where(digit_mask)[0]\n",
    "    perm = torch.randperm(len(digit_indices))[:samples_per_digit]\n",
    "    train_indices.extend(digit_indices[perm].tolist())\n",
    "\n",
    "train_indices = torch.tensor(train_indices)\n",
    "X_train = images[train_indices]\n",
    "y_train = labels[train_indices]\n",
    "\n",
    "# Sample eval set (for visualization)\n",
    "eval_indices = []\n",
    "eval_per_digit = EVAL_SIZE // 10\n",
    "for d in range(10):\n",
    "    digit_mask = test_labels == d\n",
    "    digit_indices = torch.where(digit_mask)[0]\n",
    "    perm = torch.randperm(len(digit_indices))[:eval_per_digit]\n",
    "    eval_indices.extend(digit_indices[perm].tolist())\n",
    "\n",
    "eval_indices = torch.tensor(eval_indices)\n",
    "X_eval = test_images[eval_indices]\n",
    "y_eval = test_labels[eval_indices]\n",
    "\n",
    "print(f\"Train: {len(X_train)} images\")\n",
    "print(f\"Eval: {len(X_eval)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Raw Pixel Clustering\n",
    "\n",
    "First, cluster raw pixels by cosine similarity to see baseline structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images to vectors\n",
    "X_flat = X_eval.view(len(X_eval), -1).cpu().numpy()\n",
    "y_np = y_eval.cpu().numpy()\n",
    "\n",
    "# Normalize to unit length (for cosine similarity)\n",
    "X_norm = X_flat / (np.linalg.norm(X_flat, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "# K-means clustering (k=10 for 10 digits)\n",
    "kmeans = KMeans(n_clusters=10, random_state=SEED, n_init=10)\n",
    "pixel_clusters = kmeans.fit_predict(X_norm)\n",
    "\n",
    "# Metrics\n",
    "silhouette = silhouette_score(X_norm, pixel_clusters)\n",
    "ari = adjusted_rand_score(y_np, pixel_clusters)\n",
    "\n",
    "print(f\"Raw Pixels - Silhouette: {silhouette:.3f}, ARI: {ari:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with t-SNE\n",
    "print(\"Computing t-SNE for raw pixels...\")\n",
    "tsne = TSNE(n_components=2, random_state=SEED, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_norm)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Color by true label\n",
    "scatter = axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_np, cmap='tab10', s=20, alpha=0.6)\n",
    "axes[0].set_title('Raw Pixels (colored by true digit)', fontsize=14)\n",
    "axes[0].set_xlabel('t-SNE 1')\n",
    "axes[0].set_ylabel('t-SNE 2')\n",
    "plt.colorbar(scatter, ax=axes[0], ticks=range(10))\n",
    "\n",
    "# Color by cluster\n",
    "scatter = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=pixel_clusters, cmap='tab10', s=20, alpha=0.6)\n",
    "axes[1].set_title(f'K-Means Clusters (ARI={ari:.3f})', fontsize=14)\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "plt.colorbar(scatter, ax=axes[1], ticks=range(10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Layer Activation Extraction\n",
    "\n",
    "Extract activations at each layer during training to see how representations evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook to capture layer activations\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "def extract_layer_activations(model, x):\n",
    "    \"\"\"Extract activations at each layer.\"\"\"\n",
    "    global activations\n",
    "    activations = {}\n",
    "    \n",
    "    # Register hooks\n",
    "    hooks = []\n",
    "    hooks.append(model.conv1.register_forward_hook(get_activation('conv1')))\n",
    "    hooks.append(model.conv2.register_forward_hook(get_activation('conv2')))\n",
    "    hooks.append(model.conv3.register_forward_hook(get_activation('conv3')))\n",
    "    hooks.append(model.fc1.register_forward_hook(get_activation('fc1')))\n",
    "    hooks.append(model.fc2.register_forward_hook(get_activation('fc2')))\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        _ = model(x)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    \n",
    "    # Flatten activations\n",
    "    result = {}\n",
    "    for name, act in activations.items():\n",
    "        result[name] = act.view(len(x), -1).cpu().numpy()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and save layer activations at checkpoints\n",
    "model = SmallCNN().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "layer_snapshots = []  # (epoch, layer_name, activations)\n",
    "training_history = []  # (epoch, train_acc, test_acc)\n",
    "\n",
    "print(\"Training model and capturing layer activations...\")\n",
    "for epoch in tqdm(range(MAX_EPOCHS)):\n",
    "    # Training step\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = F.cross_entropy(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_preds = model(X_train).argmax(dim=1)\n",
    "        train_acc = (train_preds == y_train).float().mean().item()\n",
    "        \n",
    "        test_preds = model(X_eval).argmax(dim=1)\n",
    "        test_acc = (test_preds == y_eval).float().mean().item()\n",
    "    \n",
    "    training_history.append((epoch + 1, train_acc, test_acc))\n",
    "    \n",
    "    # Save layer activations at checkpoints\n",
    "    if (epoch + 1) in CHECKPOINT_EPOCHS:\n",
    "        layer_acts = extract_layer_activations(model, X_eval)\n",
    "        for layer_name, acts in layer_acts.items():\n",
    "            layer_snapshots.append((epoch + 1, layer_name, acts))\n",
    "        print(f\"  Epoch {epoch + 1}: Train={train_acc:.3f}, Test={test_acc:.3f}\")\n",
    "\n",
    "print(f\"\\nFinal: Train={train_acc:.3f}, Test={test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "epochs, train_accs, test_accs = zip(*training_history)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_accs, label='Train', linewidth=2)\n",
    "plt.plot(epochs, test_accs, label='Test', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Layer-wise Clustering Over Time\n",
    "\n",
    "Visualize how each layer learns to separate digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clustering metrics for all snapshots\n",
    "layer_names = ['conv1', 'conv2', 'conv3', 'fc1', 'fc2']\n",
    "checkpoint_epochs_actual = sorted(set(e for e, _, _ in layer_snapshots))\n",
    "\n",
    "clustering_metrics = {layer: {'epoch': [], 'silhouette': [], 'ari': []} \n",
    "                     for layer in layer_names}\n",
    "\n",
    "print(\"Computing clustering metrics...\")\n",
    "for epoch, layer_name, acts in tqdm(layer_snapshots):\n",
    "    # Normalize activations\n",
    "    acts_norm = acts / (np.linalg.norm(acts, axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    # K-means clustering\n",
    "    kmeans = KMeans(n_clusters=10, random_state=SEED, n_init=10)\n",
    "    clusters = kmeans.fit_predict(acts_norm)\n",
    "    \n",
    "    # Metrics\n",
    "    silhouette = silhouette_score(acts_norm, clusters)\n",
    "    ari = adjusted_rand_score(y_np, clusters)\n",
    "    \n",
    "    clustering_metrics[layer_name]['epoch'].append(epoch)\n",
    "    clustering_metrics[layer_name]['silhouette'].append(silhouette)\n",
    "    clustering_metrics[layer_name]['ari'].append(ari)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clustering quality over time\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Silhouette score\n",
    "for layer in layer_names:\n",
    "    axes[0].plot(clustering_metrics[layer]['epoch'], \n",
    "                clustering_metrics[layer]['silhouette'],\n",
    "                marker='o', label=layer, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Silhouette Score')\n",
    "axes[0].set_title('Cluster Cohesion Over Training', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Adjusted Rand Index\n",
    "for layer in layer_names:\n",
    "    axes[1].plot(clustering_metrics[layer]['epoch'], \n",
    "                clustering_metrics[layer]['ari'],\n",
    "                marker='o', label=layer, linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Adjusted Rand Index')\n",
    "axes[1].set_title('Alignment with True Labels Over Training', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Layer Visualization Grid\n",
    "\n",
    "Show t-SNE of each layer at different epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize ALL layers at ALL epochs - create frames for animation\nvis_epochs = sorted(set(e for e, _, _ in layer_snapshots))\nvis_layers = ['conv1', 'conv2', 'conv3', 'fc1', 'fc2']\n\n# Pre-compute all t-SNE embeddings\nprint(\"Pre-computing t-SNE embeddings for all layers and epochs...\")\nembeddings = {}  # (epoch, layer) -> embedding\n\nfor epoch, layer, acts in tqdm(layer_snapshots):\n    # Normalize\n    acts_norm = acts / (np.linalg.norm(acts, axis=1, keepdims=True) + 1e-8)\n    \n    # Use PCA first if dimensionality is very high\n    if acts_norm.shape[1] > 50:\n        pca = PCA(n_components=50, random_state=SEED)\n        acts_pca = pca.fit_transform(acts_norm)\n    else:\n        acts_pca = acts_norm\n    \n    # t-SNE\n    tsne = TSNE(n_components=2, random_state=SEED, perplexity=30)\n    embedding = tsne.fit_transform(acts_pca)\n    embeddings[(epoch, layer)] = embedding\n\nprint(f\"Computed {len(embeddings)} embeddings\")"
  },
  {
   "cell_type": "code",
   "source": "# Also create static grid visualization of ALL layers\nprint(\"Creating static grid of all layers...\")\n\n# Select specific epochs to show\ngrid_epochs = [0, 2, 5, 10, 20, 50, 100]\ngrid_layers = ['conv1', 'conv2', 'conv3', 'fc1', 'fc2']\n\nfig, axes = plt.subplots(len(grid_epochs), len(grid_layers), \n                         figsize=(4 * len(grid_layers), 4 * len(grid_epochs)))\n\nfor i, epoch in enumerate(grid_epochs):\n    for j, layer in enumerate(grid_layers):\n        ax = axes[i, j]\n        \n        # Get embedding\n        embedding = embeddings.get((epoch, layer))\n        if embedding is None:\n            ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n            ax.set_xticks([])\n            ax.set_yticks([])\n            continue\n        \n        # Get ARI\n        if epoch in clustering_metrics[layer]['epoch']:\n            ari_idx = clustering_metrics[layer]['epoch'].index(epoch)\n            ari = clustering_metrics[layer]['ari'][ari_idx]\n        else:\n            ari = 0.0\n        \n        # Plot\n        scatter = ax.scatter(embedding[:, 0], embedding[:, 1], \n                           c=y_np, cmap='tab10', s=8, alpha=0.6)\n        ax.set_title(f'{layer} @ Epoch {epoch} (ARI={ari:.2f})', fontsize=10)\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n# Add row labels\nfor i, epoch in enumerate(grid_epochs):\n    axes[i, 0].set_ylabel(f'Epoch {epoch}', fontsize=12, rotation=0, \n                         ha='right', va='center', labelpad=40)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Download GIF using iframe (only method that works in VS Code Colab)\nfrom IPython.display import HTML, display\nimport base64\n\n# Read the GIF file\nwith open('/tmp/latent_evolution.gif', 'rb') as f:\n    gif_data = f.read()\n\n# Encode as base64\nb64 = base64.b64encode(gif_data).decode()\n\n# Create download link\nhtml = f'''\n<a download=\"latent_evolution.gif\" \n   href=\"data:image/gif;base64,{b64}\" \n   id=\"download_link\">\n   Click to download latent_evolution.gif\n</a>\n<script>\n// Auto-trigger download\ndocument.getElementById('download_link').click();\n</script>\n<div style=\"margin-top: 1rem;\">\n<img src=\"data:image/gif;base64,{b64}\" style=\"max-width: 100%;\"/>\n</div>\n'''\ndisplay(HTML(html))\nprint(\"GIF download should start automatically. Preview shown above.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create animation frames\nimport matplotlib\nmatplotlib.use('Agg')  # Non-interactive backend for rendering\nfrom matplotlib.animation import FuncAnimation, PillowWriter\nimport io\n\nprint(\"Creating animation frames...\")\nfig, axes = plt.subplots(1, len(vis_layers), figsize=(4 * len(vis_layers), 4))\n\ndef update_frame(epoch):\n    for j, layer in enumerate(vis_layers):\n        ax = axes[j]\n        ax.clear()\n        \n        # Get embedding\n        embedding = embeddings.get((epoch, layer))\n        if embedding is None:\n            ax.text(0.5, 0.5, f'Epoch {epoch}\\nNo data', \n                   ha='center', va='center', transform=ax.transAxes)\n            continue\n        \n        # Get ARI\n        ari_idx = clustering_metrics[layer]['epoch'].index(epoch)\n        ari = clustering_metrics[layer]['ari'][ari_idx]\n        \n        # Plot\n        scatter = ax.scatter(embedding[:, 0], embedding[:, 1], \n                           c=y_np, cmap='tab10', s=10, alpha=0.7)\n        ax.set_title(f'{layer} (Epoch {epoch}, ARI={ari:.3f})', fontsize=11)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    fig.tight_layout()\n    return axes\n\n# Create animation\nanim = FuncAnimation(fig, update_frame, frames=vis_epochs, interval=200, repeat=True)\n\n# Save as GIF\nprint(\"Saving animation as GIF (this may take a while)...\")\nwriter = PillowWriter(fps=5)\nanim.save('/tmp/latent_evolution.gif', writer=writer, dpi=80)\n\nplt.close(fig)\nprint(\"Animation saved!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "Compare final layer to raw pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CLUSTERING QUALITY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nRaw Pixels:\")\n",
    "print(f\"  Silhouette: {silhouette:.3f}\")\n",
    "print(f\"  ARI: {ari:.3f}\")\n",
    "\n",
    "print(f\"\\nFinal Epoch ({MAX_EPOCHS}):\")\n",
    "for layer in layer_names:\n",
    "    final_sil = clustering_metrics[layer]['silhouette'][-1]\n",
    "    final_ari = clustering_metrics[layer]['ari'][-1]\n",
    "    print(f\"  {layer:6s}: Silhouette={final_sil:.3f}, ARI={final_ari:.3f}\")\n",
    "\n",
    "# Best layer\n",
    "best_layer = max(layer_names, key=lambda l: clustering_metrics[l]['ari'][-1])\n",
    "best_ari = clustering_metrics[best_layer]['ari'][-1]\n",
    "print(f\"\\nBest layer: {best_layer} (ARI={best_ari:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}